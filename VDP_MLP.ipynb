{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VDP_MLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimahdera/VDP_MLP/blob/master/VDP_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jPD7VBEWKi38",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "#tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHCATIEliF8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GdrduaXMgk6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ccd787f-a317-4bd5-dd66-ee37e252041c"
      },
      "source": [
        "print(tf.__version__)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wniYrsLiOgkl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "3268987e-9769-44c2-d612-5d093ef62813"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(x_train.shape[0], -1) / 255\n",
        "x_test = x_test.reshape(x_test.shape[0], -1) / 255\n",
        "\n",
        "batch_size = 10 # MUST HAVE ALL BATCHES of EQUAL SIZE (The last one may mess some )\n",
        "\n",
        "tr_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuzNmCzTz4-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(x_train.shape[0], -1) / 255\n",
        "x_test = x_test.reshape(x_test.shape[0], -1) / 255\n",
        "\n",
        "batch_size = 10 # MUST HAVE ALL BATCHES of EQUAL SIZE (The last one may mess some )\n",
        "\n",
        "one_hot_y_train = tf.one_hot(y_train.astype(np.float32), depth=10)\n",
        "one_hot_y_test = tf.one_hot(y_test.astype(np.float32), depth=10)\n",
        "\n",
        "\n",
        "tr_dataset = tf.data.Dataset.from_tensor_slices((x_train, one_hot_y_train)).batch(batch_size)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, one_hot_y_test)).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilRiLniO_Y1k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74b427fe-e3ad-4569-e33a-5fc451369c6a"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rliXMqE0pAQ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ba52eea-2a20-4ef5-8fe9-36955309d4b8"
      },
      "source": [
        "tr_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((None, 784), (None, 10)), types: (tf.float64, tf.float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhcOjmfmYIHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def x_Sigma_w_x_T(x, W_Sigma):\n",
        "  batch_sz = x.shape[0]\n",
        "  xx_t = tf.reduce_sum(tf.multiply(x, x),axis=1, keepdims=True)               # xxT is being calcualted\n",
        "  xx_t_e = tf.expand_dims(xx_t,axis=2)                                      # Expand dimention of xxt\n",
        "  return tf.multiply(xx_t_e, W_Sigma)\n",
        "\n",
        "def w_t_Sigma_i_w (w_mu, in_Sigma):\n",
        "  Sigma_1_1 = tf.matmul(tf.transpose(w_mu), in_Sigma)\n",
        "  Sigma_1_2 = tf.matmul(Sigma_1_1, w_mu)\n",
        "  return Sigma_1_2\n",
        "\n",
        "def tr_Sigma_w_Sigma_in (in_Sigma, W_Sigma):\n",
        "  Sigma_3_1 = tf.linalg.trace(in_Sigma)\n",
        "  Sigma_3_2 = tf.expand_dims(Sigma_3_1, axis=1)\n",
        "  Sigma_3_3 = tf.expand_dims(Sigma_3_2, axis=1)\n",
        "  return tf.multiply(Sigma_3_3, W_Sigma) \n",
        "\n",
        "def activation_Sigma (gradi, Sigma_in):\n",
        "  grad1 = tf.expand_dims(gradi,axis=2)\n",
        "  grad2 = tf.expand_dims(gradi,axis=1)\n",
        "  return tf.multiply(Sigma_in, tf.matmul(grad1, grad2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOX8GxumKknc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Linear Class - First Layer (Constant * RV)\n",
        "class LinearFirst(keras.layers.Layer):\n",
        "    \"\"\"y = w.x + b\"\"\"\n",
        "\n",
        "    def __init__(self, units):\n",
        "        super(LinearFirst, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w_mu = self.add_weight(name='w_mu',\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None),\n",
        "            trainable=True\n",
        "        )\n",
        "        self.w_sigma = self.add_weight(name='w_sigma',\n",
        "            shape=(self.units,),\n",
        "            initializer=tf.random_uniform_initializer(minval= -12., maxval=-2.2, seed=None),\n",
        "            trainable=True\n",
        "        )\n",
        "        self.b_mu = self.add_weight(name='b_mu',\n",
        "            shape=(self.units,), initializer=tf.random_normal_initializer( mean=0.0, stddev=0.00005, seed=None),\n",
        "            trainable=True\n",
        "        )\n",
        "        self.b_sigma = self.add_weight(name='b_sigma',\n",
        "            shape=(self.units,), initializer=tf.random_uniform_initializer(minval= -12., maxval=-10., seed=None),\n",
        "            trainable=True\n",
        "        )\n",
        "    def call(self, inputs):\n",
        "        # Mean\n",
        "        #print(self.w_mu.shape)\n",
        "        mu_out = tf.matmul(inputs, self.w_mu) + self.b_mu                         # Mean of the output\n",
        "        # Varinace\n",
        "        W_Sigma = tf.linalg.diag(tf.math.log(1. + tf.math.exp(self.w_sigma)))                                    # Construct W_Sigma from w_sigmas\n",
        "        #W_Sigma = tf.linalg.diag(self.w_sigma)                                    # Construct W_Sigma from w_sigmas\n",
        "        Sigma_out = x_Sigma_w_x_T(inputs, W_Sigma) + tf.math.log(1. + tf.math.exp(self.b_sigma)) #tf.linalg.diag(self.b_sigma)\n",
        "\n",
        "        Term1 = tf.reduce_mean(tf.math.log(1.0 + Sigma_out))\n",
        "        Term2 = tf.reduce_mean(tf.square(mu_out))\n",
        "        Term3 = tf.reduce_mean(Sigma_out)\n",
        "      \n",
        "        kl_loss = -0.5 * (Term1 - Term2 - Term3)\n",
        "        self.add_loss(kl_loss)\n",
        "\n",
        "        return mu_out, Sigma_out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jOCRKggQf07i",
        "colab": {}
      },
      "source": [
        "# Linear Class - Second Layer (RV * RV)\n",
        "class LinearNotFirst(keras.layers.Layer):\n",
        "    \"\"\"y = w.x + b\"\"\"\n",
        "\n",
        "    def __init__(self, units):\n",
        "        super(LinearNotFirst, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w_mu = self.add_weight(name='w_mu',\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=tf.random_normal_initializer( mean=0.0, stddev=0.05, seed=None),\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.w_sigma = self.add_weight(name='w_sigma',\n",
        "            shape=(self.units,),\n",
        "            initializer=tf.random_uniform_initializer(minval= -12., maxval=-2.2, seed=None),\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b_mu = self.add_weight(name='b_mu',\n",
        "            shape=(self.units,), initializer=tf.random_normal_initializer( mean=0.0, stddev=0.00005, seed=None),\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b_sigma = self.add_weight(name='b_sigma',\n",
        "            shape=(self.units,), initializer=tf.random_uniform_initializer(minval= -12., maxval=-10., seed=None),\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def call(self, mu_in, Sigma_in):\n",
        "        #batch_sz = mu_in.shape[0]\n",
        "        mu_out = tf.matmul(mu_in, self.w_mu) + self.b_mu\n",
        "        W_Sigma = tf.linalg.diag(tf.math.log(1. + tf.math.exp(self.w_sigma)))  \n",
        "        #W_Sigma = tf.linalg.diag(self.w_sigma)                                    # Construct W_Sigma from w_sigmas\n",
        "        # Simga_out has three terms\n",
        "        Sigma_1 = w_t_Sigma_i_w (self.w_mu, Sigma_in)\n",
        "        Sigma_2 = x_Sigma_w_x_T(mu_in, W_Sigma)                                   #tf.keras.backend.print_tensor(inputs2[0,:,:])\n",
        "        Sigma_3 = tr_Sigma_w_Sigma_in (Sigma_in, W_Sigma)\n",
        "        Sigma_out = Sigma_1 + Sigma_2 + Sigma_3 +tf.linalg.diag(tf.math.log(1. + tf.math.exp(self.b_sigma)))  #+ tf.linalg.diag(self.b_sigma)\n",
        "        \n",
        "        Term1 = tf.reduce_mean(tf.math.log(1.0 + Sigma_out))\n",
        "        Term2 = tf.reduce_mean(tf.square(mu_out))\n",
        "        Term3 = tf.reduce_mean(Sigma_out)\n",
        "        kl_loss = -0.5 * (Term1 - Term2 - Term3)\n",
        "        self.add_loss(kl_loss)\n",
        "        return mu_out, Sigma_out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN0vKyGVSmV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myReLU(keras.layers.Layer):\n",
        "    \"\"\"ReLU\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(myReLU, self).__init__()\n",
        "    def call(self, mu_in, Sigma_in):\n",
        "        mu_out = tf.nn.relu(mu_in)\n",
        "        with tf.GradientTape() as g:\n",
        "          g.watch(mu_in)\n",
        "          out = tf.nn.relu(mu_in)\n",
        "        gradi = g.gradient(out, mu_in) \n",
        "\n",
        "        Sigma_out = activation_Sigma (gradi, Sigma_in)\n",
        "        \n",
        "        return mu_out, Sigma_out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kge0TL8dbb88",
        "colab": {}
      },
      "source": [
        "class mysoftmax(keras.layers.Layer):\n",
        "    \"\"\"Mysoftmax\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(mysoftmax, self).__init__()\n",
        "    def call(self, mu_in, Sigma_in):\n",
        "        mu_out = tf.nn.softmax(mu_in)\n",
        "        pp1 = tf.expand_dims(mu_out, axis=2)\n",
        "        pp2 = tf.expand_dims(mu_out, axis=1)\n",
        "        ppT = tf.matmul(pp1, pp2)\n",
        "        p_diag = tf.linalg.diag(mu_out)\n",
        "        grad = p_diag - ppT\n",
        "        Sigma_out = tf.matmul(grad, tf.matmul(Sigma_in, tf.transpose(grad, perm=[0, 2, 1])))\n",
        "        return mu_out, Sigma_out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "162UvEyOKrtM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "054f70e2-0818-468a-f0d9-e135536baef7"
      },
      "source": [
        "# Build Model using Model API\n",
        "\n",
        "inputs = tf.keras.Input(shape=(784,),batch_size=batch_size, dtype=\"float32\")\n",
        "m, s = LinearFirst(256)(inputs)\n",
        "m, s = myReLU()(m, s)\n",
        "m, s = LinearNotFirst(10)(m, s)\n",
        "outputs, Sigma = mysoftmax()(m, s)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.summary(line_length=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "____________________________________________________________________________________________________\n",
            "Layer (type)                                 Output Shape                            Param #        \n",
            "====================================================================================================\n",
            "input_1 (InputLayer)                         [(200, 784)]                            0              \n",
            "____________________________________________________________________________________________________\n",
            "linear_first (LinearFirst)                   ((200, 256), (200, 256, 256))           201472         \n",
            "____________________________________________________________________________________________________\n",
            "my_re_lu (myReLU)                            ((200, 256), (200, 256, 256))           0              \n",
            "____________________________________________________________________________________________________\n",
            "linear_not_first (LinearNotFirst)            ((200, 10), (200, 10, 10))              2590           \n",
            "____________________________________________________________________________________________________\n",
            "mysoftmax (mysoftmax)                        ((200, 10), (200, 10, 10))              0              \n",
            "====================================================================================================\n",
            "Total params: 204,062\n",
            "Trainable params: 204,062\n",
            "Non-trainable params: 0\n",
            "____________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IF_N04ksM8Ob",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "f993bb61-f0a7-4853-e2ff-5d84c086c660"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
        ")\n",
        "history = model.fit(tr_dataset, epochs=10, validation_data=val_dataset)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.9590 - acc: 0.9803 - val_loss: 0.9823 - val_acc: 0.9689\n",
            "Epoch 2/10\n",
            "300/300 [==============================] - 3s 8ms/step - loss: 0.9513 - acc: 0.9821 - val_loss: 0.9801 - val_acc: 0.9698\n",
            "Epoch 3/10\n",
            "300/300 [==============================] - 3s 8ms/step - loss: 0.9461 - acc: 0.9834 - val_loss: 0.9781 - val_acc: 0.9695\n",
            "Epoch 4/10\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.9416 - acc: 0.9846 - val_loss: 0.9765 - val_acc: 0.9697\n",
            "Epoch 5/10\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.9381 - acc: 0.9855 - val_loss: 0.9743 - val_acc: 0.9704\n",
            "Epoch 6/10\n",
            "300/300 [==============================] - 3s 8ms/step - loss: 0.9346 - acc: 0.9863 - val_loss: 0.9740 - val_acc: 0.9706\n",
            "Epoch 7/10\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.9319 - acc: 0.9872 - val_loss: 0.9729 - val_acc: 0.9710\n",
            "Epoch 8/10\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.9289 - acc: 0.9878 - val_loss: 0.9721 - val_acc: 0.9710\n",
            "Epoch 9/10\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.9264 - acc: 0.9884 - val_loss: 0.9714 - val_acc: 0.9717\n",
            "Epoch 10/10\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.9245 - acc: 0.9890 - val_loss: 0.9707 - val_acc: 0.9720\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx5g3olnWto4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV48oPo1P87u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #############################\n",
        "###### Cutom training loop\n",
        "class exVDPMLP(tf.keras.Model):\n",
        "    \"\"\"Stack of Linear layers with a KL regularization loss.\"\"\"\n",
        "\n",
        "    def __init__(self, name=None):\n",
        "        super(exVDPMLP, self).__init__()\n",
        "        self.linear_1 = LinearFirst(256)\n",
        "        self.myrelu_1 = myReLU()\n",
        "        self.linear_2 = LinearNotFirst(10)\n",
        "        self.mysoftma = mysoftmax()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        m, s = self.linear_1 (inputs)\n",
        "        m, s = self.myrelu_1 (m, s)\n",
        "        m, s = self.linear_2 (m, s)\n",
        "        outputs, Sigma = self.mysoftma(m, s)\n",
        "        #print('Sample Sigma', Sigma[0,:,:])\n",
        "        return outputs, Sigma\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzabXnqUaIsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp = exVDPMLP()\n",
        "loss_total=[]\n",
        "loss_total_layer=[]\n",
        "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "for epoch in range(2):\n",
        "  for step, (x, y) in enumerate(tr_dataset):\n",
        "\n",
        "    # Open a GradientTape.\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        # Forward pass.\n",
        "        logits = mlp(x)\n",
        "\n",
        "        # Loss value for this batch.\n",
        "        loss_final = loss_fn(y, logits)\n",
        "        loss_layers = sum(mlp.losses)\n",
        "\n",
        "        loss = loss_final + loss_layers\n",
        "\n",
        "    # Update the state of the `accuracy` metric.\n",
        "    accuracy.update_state(y, logits)\n",
        "    \n",
        "    # Get gradients of weights wrt the loss.\n",
        "    gradients = tape.gradient(loss, mlp.trainable_weights)\n",
        "\n",
        "    # Update the weights of our linear layer.\n",
        "    optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n",
        "#    if step==1:\n",
        "#       break\n",
        "    # Logging.\n",
        "    if step % 100 == 0:\n",
        "        print(\"Step:\", step, \"Loss:\", float(loss))\n",
        "        print(\"Total running accuracy so far: %.3f\" % accuracy.result())\n",
        "        loss_total.append(loss)\n",
        "        loss_total_layer.append(loss_layers)\n",
        "  accuracy.reset_states()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUaJeG4GzWPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nll_gaussian(y_test, y_pred_mean, y_pred_sd, num_labels=10, batch_size=10):\n",
        "    NS = tf.linalg.diag(tf.constant(1e-3, shape=[batch_size, num_labels]))\n",
        "    I = tf.eye(num_labels, batch_shape=[batch_size])\n",
        "    y_pred_sd_ns = y_pred_sd + NS\n",
        "    y_pred_sd_inv = tf.linalg.solve(y_pred_sd_ns, I)\n",
        "    mu_ = y_pred_mean - y_test\n",
        "   # return tf.reduce_mean(mu_**2)\n",
        "    mu_sigma = tf.matmul(mu_ ,  y_pred_sd_inv) \n",
        "    ms = 0.5*tf.matmul(mu_sigma , mu_, transpose_b=True) + 0.5*tf.linalg.slogdet(y_pred_sd_ns)[1]\n",
        "    \n",
        "    ms = tf.reduce_mean(ms)\n",
        "    return(ms)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxmKXiTKyiq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cutom Trianing Loop with Graph\n",
        "mlp = exVDPMLP(name='mlp')\n",
        "#accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "#loss_fn = NLL_loss(num_labels=10, batch_size=100)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_total=[]\n",
        "@tf.function  # Make it fast.\n",
        "def train_on_batch(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits, sigma = mlp(x)\n",
        "        \n",
        "       # loss_final = loss_fn( y, logits, sigma)\n",
        "        loss_final = nll_gaussian(y, logits,   tf.clip_by_value(t=sigma, clip_value_min=tf.constant(-1e+10),\n",
        "                                  clip_value_max=tf.constant(1e+10)))\n",
        "        #loss_layers = sum(gru_model.losses)\n",
        "\n",
        "        loss = loss_final #+ loss_layers\n",
        "\n",
        "        gradients = tape.gradient(loss, mlp.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n",
        "    #acc = accuracy.update_state(y, logits)\n",
        "    return loss, logits\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "  print('Epoch: ', epoch, '/' , epochs)\n",
        "  for step, (x, y) in enumerate(tr_dataset):\n",
        "    loss, logits = train_on_batch(x, y)\n",
        "   # logits1 = tf.math.argmax( logits, axis=1)\n",
        "   # y1 = tf.math.argmax( y, axis=1)\n",
        "\n",
        "    corr = tf.equal(tf.math.argmax(logits, 1),tf.math.argmax(y,1))   \n",
        "    accuracy = tf.reduce_mean(tf.cast(corr,tf.float32))\n",
        "\n",
        "  #  print(logits1)\n",
        "   # print(y1)\n",
        "   # accuracy.update_state(y1, logits1)\n",
        "    if step % 1000 == 0:\n",
        "        print(\"Step:\", step, \"Loss:\", float(loss))\n",
        "        print(\"Total running accuracy so far: %.3f\" % accuracy)\n",
        "        loss_total.append(loss)\n",
        "  for step, (x, y) in enumerate(val_dataset):\n",
        "    logits, sigma = mlp(x)\n",
        "    corr = tf.equal(tf.math.argmax(logits, 1),tf.math.argmax(y,1))   \n",
        "    va_accuracy = tf.reduce_mean(tf.cast(corr,tf.float32))\n",
        "    if step % 1 == 0:\n",
        "      print(\"Total running accuracy so far: %.3f\" % va_accuracy)\n",
        "       # loss_total_layer.append(loss_layers)\n",
        "  #accuracy.reset_states()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIWV6uyKqkTR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d68d45c8-7a6c-4714-b420-eb03ad67fd9d"
      },
      "source": [
        "!mkdir -p saved_model2\n",
        "#tf.saved_model.save(gru_model, '/saved_model/my_model')\n",
        "mlp.save('saved_model2/my_model') \n",
        "#mlp.save_weights('saved_model/my_model', save_format='tf') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model2/my_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mU-1qwCpFAJE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "e76c7b4c-023c-4bae-b61e-1d639cf0b1cf"
      },
      "source": [
        "!mkdir -p saved_model2\n",
        "tf.saved_model.save(mlp, '/saved_model2/saved_model2')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: /saved_model2/saved_model2/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVPiJWPp3P34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp.save_weights('./saved_model2/my_checkpoint')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNflVF15cFMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp.save_weights('./saved_model/my_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu0s2b8esx7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model():\n",
        "  return exVDPMLP(name='mlp')\n",
        "model = get_model()  \n",
        "model.save_weights('./saved_model/my_model', save_format='tf')\n",
        "#model.save_weights('./saved_model/my_model.h5')#, save_format='tf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXDi2oDh-hqv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bad39b9a-cf48-4610-8e4c-e5eeb5f2f871"
      },
      "source": [
        "def get_model():\n",
        "  return exVDPMLP(name='mlp')\n",
        "newmodel = get_model()  \n",
        "#newmodel.load_weights('./saved_model/my_model.h5')\n",
        "newmodel.load_weights('./saved_model2/my_checkpoint')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe8a9b69da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ0nEODZvEbH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6d4a822c-dd79-4bec-ad56-550d8ccea231"
      },
      "source": [
        "#mlp = exVDPMLP()\n",
        "for step, (x, y) in enumerate(val_dataset):\n",
        "  logits, sigma = newmodel(x)\n",
        "  corr = tf.equal(tf.math.argmax(logits, 1),tf.math.argmax(y,1))  \n",
        "  accuracy = tf.reduce_mean(tf.cast(corr,tf.float32))\n",
        "  if step % 1 == 0:\n",
        "    print(\"Step:\", step, \"Loss:\", float(loss))\n",
        "    print(\"Total running accuracy so far: %.3f\" % accuracy)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer ex_vdpmlp_14 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Step: 0 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 1 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 2 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 3 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 4 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 5 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 6 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 7 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 8 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 9 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 10 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 11 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 12 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 13 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 14 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 15 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 16 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 17 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 18 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 19 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 20 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 21 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 22 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 23 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 24 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 25 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 26 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 27 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 28 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 29 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 30 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 31 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 32 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 33 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 34 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 35 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 36 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 37 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 38 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 39 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 40 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 41 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 42 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 43 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 44 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.700\n",
            "Step: 45 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 46 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 47 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 48 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 49 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 50 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 51 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 52 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 53 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 54 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 55 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 56 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 57 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 58 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 59 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 60 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 61 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 62 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 63 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 64 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 65 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 66 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 67 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 68 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 69 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 70 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 71 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 72 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 73 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 74 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 75 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 76 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 77 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 78 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 79 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 80 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 81 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 82 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 83 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 84 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 85 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 86 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 87 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 88 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 89 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 90 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 91 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 92 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 93 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 94 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 95 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 96 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 97 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 98 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 99 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 100 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 101 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 102 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 103 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 104 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 105 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 106 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 107 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 108 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 109 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 110 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 111 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 112 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 113 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 114 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 115 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 116 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 117 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 118 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 119 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 120 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 121 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 122 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 123 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.800\n",
            "Step: 124 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.800\n",
            "Step: 125 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 126 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 127 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 128 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 129 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 130 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 131 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 132 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 133 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 134 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 135 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 136 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 137 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 138 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 139 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 140 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 141 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 142 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 143 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 144 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 145 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 146 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 147 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 148 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 149 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 150 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 151 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 152 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 153 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 154 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 155 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 156 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 157 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 158 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 159 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 160 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 161 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 162 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 163 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 164 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 165 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 166 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 167 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 168 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 169 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 170 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 171 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 172 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 173 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 174 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 175 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 176 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 177 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 178 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 179 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 180 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 181 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 182 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 183 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 184 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 185 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 186 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 187 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 188 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 189 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 190 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 191 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 192 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 193 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 194 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 195 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 196 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 197 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 198 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 199 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 200 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 201 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 202 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 203 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 204 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 205 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 206 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 207 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 208 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 209 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.800\n",
            "Step: 210 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 211 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 212 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 213 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.800\n",
            "Step: 214 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 215 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 216 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 217 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 218 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 219 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 220 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 221 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 222 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 223 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 224 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 225 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 226 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 227 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 228 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 229 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.700\n",
            "Step: 230 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 231 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 232 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 233 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 234 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 235 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 236 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 237 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 238 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 239 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 240 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 241 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 242 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 243 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 244 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 245 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 246 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 247 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 248 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 249 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 250 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 251 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 252 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 253 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 254 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 255 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 256 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 257 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 258 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 259 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 260 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 261 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 262 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 263 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 264 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 265 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 266 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 267 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 268 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 269 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 270 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 271 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 272 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 273 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 274 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 275 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 276 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 277 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 278 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 279 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 280 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 281 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 282 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 283 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 284 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 285 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 286 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 287 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 288 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 289 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 290 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 291 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 292 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.800\n",
            "Step: 293 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 294 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 295 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 296 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 297 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 298 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 299 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 300 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 301 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 302 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 303 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 304 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 305 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 306 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 307 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 308 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 309 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 310 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 311 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 312 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 313 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 314 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 315 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 316 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 317 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 318 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 319 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 320 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 321 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 322 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 323 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 324 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 325 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 326 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 327 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 328 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 329 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 330 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 331 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 332 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 333 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.800\n",
            "Step: 334 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 335 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 336 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 337 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 338 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 339 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 340 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 341 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 342 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 343 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 344 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 345 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 346 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 347 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 348 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 349 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 350 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 351 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 352 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 353 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 354 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 355 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.800\n",
            "Step: 356 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 357 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 358 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 359 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 360 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 361 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 362 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 363 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 364 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 365 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 366 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 367 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 368 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 369 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 370 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 371 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 372 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 373 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 374 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 375 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 376 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 377 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 378 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 379 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 380 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 381 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.800\n",
            "Step: 382 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 383 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 384 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 385 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 386 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 387 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 388 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 389 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 390 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 391 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 392 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 393 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 394 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 395 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 396 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 397 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 398 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 399 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 400 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 401 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 402 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 403 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 404 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 405 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 406 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.800\n",
            "Step: 407 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 408 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 409 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 410 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 411 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 412 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 413 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 414 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 415 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 416 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 417 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 418 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 419 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 420 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 421 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 422 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 423 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 424 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 425 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 426 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 427 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 428 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 429 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 430 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 431 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 432 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 433 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 434 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 435 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 436 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 437 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 438 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 439 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 440 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 441 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 442 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 443 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 444 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 445 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 446 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 447 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 448 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 449 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 450 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 451 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 452 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 453 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 454 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 455 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 456 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 457 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 458 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 459 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 460 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 461 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 462 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 463 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 464 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 465 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 466 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 467 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 468 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 469 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 470 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 471 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 472 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 473 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 474 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 475 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 476 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 477 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 478 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 479 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 480 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 481 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 482 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 483 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 484 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 485 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 486 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 487 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 488 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 489 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 490 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 491 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 492 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 493 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 494 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 495 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 496 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 497 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 498 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 499 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 500 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 501 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 502 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 503 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 504 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 505 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 506 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.800\n",
            "Step: 507 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 508 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 509 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 510 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 511 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 512 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 513 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 514 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 515 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 516 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 517 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 518 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 519 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 520 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 521 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 522 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 523 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 524 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 525 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 526 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 527 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 528 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 529 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 530 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 531 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 532 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 533 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 534 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 535 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 536 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 537 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 538 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 539 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 540 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 541 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 542 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 543 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 544 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 545 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 546 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 547 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 548 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 549 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 550 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 551 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 552 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 553 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 554 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 555 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 556 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 557 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 558 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 559 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 560 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 561 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 562 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 563 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 564 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 565 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 566 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 567 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 568 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 569 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 570 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 571 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 572 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 573 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 574 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 575 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 576 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 577 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 578 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 579 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 580 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 581 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 582 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 583 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 584 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 585 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 586 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 587 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 588 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.800\n",
            "Step: 589 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 590 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 591 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 592 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 593 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 594 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 595 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 596 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 597 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 598 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 599 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 600 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 601 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 602 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 603 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 604 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 605 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 606 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 607 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 608 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 609 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 610 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 611 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 612 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 613 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 614 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 615 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 616 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 617 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 618 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 619 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 620 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 621 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 622 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 623 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 624 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 625 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 626 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 627 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 628 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 629 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 630 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 631 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 632 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 633 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 634 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 635 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 636 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 637 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 638 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 639 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 640 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 641 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 642 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 643 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 644 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 645 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 646 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 647 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 648 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 649 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 650 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 651 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 652 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 653 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 654 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 655 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 656 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 657 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.800\n",
            "Step: 658 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 659 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 660 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 661 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 662 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 663 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 664 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 665 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 666 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 667 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 668 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 669 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 670 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 671 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 672 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 673 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 674 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 675 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 676 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 677 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 678 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 679 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 680 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 681 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 682 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 683 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 684 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 685 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 686 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 687 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 688 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 689 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 690 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 691 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 692 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 693 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 694 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 695 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 696 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 697 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 698 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 699 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 700 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 701 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 702 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 703 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 704 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 705 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 706 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 707 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 708 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 709 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 710 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 711 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 712 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 713 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 714 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 715 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 716 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 717 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 718 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 719 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 720 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 721 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 722 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 723 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 724 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 725 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 726 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 727 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 728 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 729 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 730 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 731 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 732 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 733 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 734 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 735 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 736 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 737 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 738 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 739 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 740 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 741 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 742 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 743 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 744 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 745 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 746 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 747 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 748 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 749 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 750 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 751 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 752 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 753 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 754 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 755 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 756 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 757 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 758 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 759 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 760 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 761 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 762 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 763 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 764 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 765 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 766 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 767 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 768 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 769 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 770 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 771 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 772 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 773 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 774 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 775 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 776 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 777 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 778 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 779 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 780 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 781 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 782 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 783 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 784 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 785 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 786 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 787 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 788 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 789 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 790 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 791 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 792 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 793 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 794 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 795 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 796 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 797 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 798 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 799 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 800 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 801 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 802 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 803 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 804 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 805 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 806 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 807 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 808 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 809 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 810 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 811 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 812 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 813 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 814 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 815 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 816 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 817 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 818 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 819 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 820 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 821 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 822 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 823 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 824 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 825 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 826 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 827 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 828 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 829 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 830 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 831 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 832 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 833 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 834 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 835 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 836 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 837 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 838 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 839 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 840 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 841 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 842 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 843 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 844 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 845 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 846 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 847 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 848 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 849 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 850 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 851 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 852 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 853 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 854 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 855 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 856 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 857 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 858 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 859 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 860 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 861 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 862 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 863 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 864 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 865 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 866 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 867 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 868 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 869 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 870 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 871 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 872 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 873 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 874 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 875 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 876 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 877 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 878 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 879 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 880 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 881 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 882 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 883 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 884 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 885 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 886 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 887 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 888 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 889 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 890 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 891 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 892 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 893 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 894 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 895 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 896 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 897 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 898 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 899 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 900 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 901 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.800\n",
            "Step: 902 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 903 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 904 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 905 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 906 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 907 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 908 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 909 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 910 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 911 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 912 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 913 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 914 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 915 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 916 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 917 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 918 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 919 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 920 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 921 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 922 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 923 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 924 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 925 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 926 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 927 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 928 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 929 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 930 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 931 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 932 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 933 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 934 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 935 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 936 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 937 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 938 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 939 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 940 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 941 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 942 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 943 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 944 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 945 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 946 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 947 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 948 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 949 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 950 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 951 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 952 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 953 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 954 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 955 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 956 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 957 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 958 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 959 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 960 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 961 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 962 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 963 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 964 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 965 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 966 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.800\n",
            "Step: 967 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 968 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 969 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 970 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 971 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 972 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 973 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 974 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.800\n",
            "Step: 975 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 976 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.800\n",
            "Step: 977 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.700\n",
            "Step: 978 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 979 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 980 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 981 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 982 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 983 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 984 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 985 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 986 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 987 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 988 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 989 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 990 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 991 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 992 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 993 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 994 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 995 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 996 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 997 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n",
            "Step: 998 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 999 Loss: -34.53819274902344\n",
            "Total running accuracy so far: 1.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG1dgBUqHlYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = newmodel.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKzgLO6TQj11",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e46d584-e299-4466-fcb7-ce3ab6a457bc"
      },
      "source": [
        "predictions[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yIA5lpAROY3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0b8f1dd0-a403-4dbe-cb07-63ad4e6915c9"
      },
      "source": [
        "y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyQWKt03t-3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr1 = np.equal(np.argmax(predictions[0], 1),y_test)   \n",
        "accuracy1 = np.mean(corr1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk2bcu-TzA-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_model = get_model()\n",
        "new_model.load_weights('./saved_model/my_model')\n",
        "predictions = new_model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYcu3ScKzSKS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76946868-8678-49a5-94bc-5324f5f2e54b"
      },
      "source": [
        "np.argmax(predictions[0],1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 4, ..., 4, 4, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yNOQGuZcW6k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "54cba6a0-410e-42b4-a9fb-fa404f31a43d"
      },
      "source": [
        "# Cutom Trianing Loop with Graph\n",
        "mlp = exVDPMLP()\n",
        "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "@tf.function  # Make it fast.\n",
        "def train_on_batch(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = mlp(x)\n",
        "        \n",
        "        loss_final = loss_fn(y, logits)\n",
        "        loss_layers = sum(mlp.losses)\n",
        "\n",
        "        loss = loss_final + loss_layers\n",
        "\n",
        "        gradients = tape.gradient(loss, mlp.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n",
        "    #acc = accuracy.update_state(y, logits)\n",
        "    return loss, logits\n",
        "\n",
        "for epoch in range(10):\n",
        "  for step, (x, y) in enumerate(tr_dataset):\n",
        "    loss, logits = train_on_batch(x, y)\n",
        "    accuracy.update_state(y, logits)\n",
        "    if step % 50 == 0:\n",
        "        print(\"Step:\", step, \"Loss:\", float(loss))\n",
        "        print(\"Total running accuracy so far: %.3f\" % accuracy.result())\n",
        "        loss_total.append(loss)\n",
        "        loss_total_layer.append(loss_layers)\n",
        "  accuracy.reset_states()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer ex_vdpmlp_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Step: 0 Loss: 2.456092357635498\n",
            "Total running accuracy so far: 0.115\n",
            "Step: 50 Loss: 2.2715015411376953\n",
            "Total running accuracy so far: 0.703\n",
            "Step: 100 Loss: 2.259002447128296\n",
            "Total running accuracy so far: 0.770\n",
            "Step: 150 Loss: 2.256253719329834\n",
            "Total running accuracy so far: 0.801\n",
            "Step: 200 Loss: 2.229085922241211\n",
            "Total running accuracy so far: 0.820\n",
            "Step: 250 Loss: 2.2208216190338135\n",
            "Total running accuracy so far: 0.832\n",
            "Step: 0 Loss: 2.21597957611084\n",
            "Total running accuracy so far: 0.885\n",
            "Step: 50 Loss: 2.2144320011138916\n",
            "Total running accuracy so far: 0.901\n",
            "Step: 100 Loss: 2.2242746353149414\n",
            "Total running accuracy so far: 0.900\n",
            "Step: 150 Loss: 2.228470802307129\n",
            "Total running accuracy so far: 0.901\n",
            "Step: 200 Loss: 2.2085556983947754\n",
            "Total running accuracy so far: 0.901\n",
            "Step: 250 Loss: 2.2046685218811035\n",
            "Total running accuracy so far: 0.902\n",
            "Step: 0 Loss: 2.201694965362549\n",
            "Total running accuracy so far: 0.925\n",
            "Step: 50 Loss: 2.2017643451690674\n",
            "Total running accuracy so far: 0.916\n",
            "Step: 100 Loss: 2.2144598960876465\n",
            "Total running accuracy so far: 0.916\n",
            "Step: 150 Loss: 2.218162775039673\n",
            "Total running accuracy so far: 0.916\n",
            "Step: 200 Loss: 2.199925422668457\n",
            "Total running accuracy so far: 0.916\n",
            "Step: 250 Loss: 2.196833848953247\n",
            "Total running accuracy so far: 0.917\n",
            "Step: 0 Loss: 2.19486141204834\n",
            "Total running accuracy so far: 0.940\n",
            "Step: 50 Loss: 2.194892644882202\n",
            "Total running accuracy so far: 0.926\n",
            "Step: 100 Loss: 2.20772647857666\n",
            "Total running accuracy so far: 0.927\n",
            "Step: 150 Loss: 2.2099204063415527\n",
            "Total running accuracy so far: 0.927\n",
            "Step: 200 Loss: 2.1950390338897705\n",
            "Total running accuracy so far: 0.927\n",
            "Step: 250 Loss: 2.192354440689087\n",
            "Total running accuracy so far: 0.927\n",
            "Step: 0 Loss: 2.1902432441711426\n",
            "Total running accuracy so far: 0.950\n",
            "Step: 50 Loss: 2.190312623977661\n",
            "Total running accuracy so far: 0.933\n",
            "Step: 100 Loss: 2.2028653621673584\n",
            "Total running accuracy so far: 0.933\n",
            "Step: 150 Loss: 2.204472541809082\n",
            "Total running accuracy so far: 0.934\n",
            "Step: 200 Loss: 2.1911568641662598\n",
            "Total running accuracy so far: 0.934\n",
            "Step: 250 Loss: 2.1886703968048096\n",
            "Total running accuracy so far: 0.934\n",
            "Step: 0 Loss: 2.1869394779205322\n",
            "Total running accuracy so far: 0.955\n",
            "Step: 50 Loss: 2.1873507499694824\n",
            "Total running accuracy so far: 0.938\n",
            "Step: 100 Loss: 2.1989641189575195\n",
            "Total running accuracy so far: 0.938\n",
            "Step: 150 Loss: 2.20038104057312\n",
            "Total running accuracy so far: 0.939\n",
            "Step: 200 Loss: 2.188524007797241\n",
            "Total running accuracy so far: 0.939\n",
            "Step: 250 Loss: 2.1859281063079834\n",
            "Total running accuracy so far: 0.939\n",
            "Step: 0 Loss: 2.1850039958953857\n",
            "Total running accuracy so far: 0.960\n",
            "Step: 50 Loss: 2.1845288276672363\n",
            "Total running accuracy so far: 0.942\n",
            "Step: 100 Loss: 2.1962993144989014\n",
            "Total running accuracy so far: 0.942\n",
            "Step: 150 Loss: 2.1973042488098145\n",
            "Total running accuracy so far: 0.943\n",
            "Step: 200 Loss: 2.185922145843506\n",
            "Total running accuracy so far: 0.943\n",
            "Step: 250 Loss: 2.184065818786621\n",
            "Total running accuracy so far: 0.943\n",
            "Step: 0 Loss: 2.183103561401367\n",
            "Total running accuracy so far: 0.960\n",
            "Step: 50 Loss: 2.1820647716522217\n",
            "Total running accuracy so far: 0.946\n",
            "Step: 100 Loss: 2.1933515071868896\n",
            "Total running accuracy so far: 0.945\n",
            "Step: 150 Loss: 2.1949563026428223\n",
            "Total running accuracy so far: 0.946\n",
            "Step: 200 Loss: 2.183713912963867\n",
            "Total running accuracy so far: 0.946\n",
            "Step: 250 Loss: 2.18278431892395\n",
            "Total running accuracy so far: 0.946\n",
            "Step: 0 Loss: 2.1816954612731934\n",
            "Total running accuracy so far: 0.960\n",
            "Step: 50 Loss: 2.1802597045898438\n",
            "Total running accuracy so far: 0.948\n",
            "Step: 100 Loss: 2.191052198410034\n",
            "Total running accuracy so far: 0.947\n",
            "Step: 150 Loss: 2.1931159496307373\n",
            "Total running accuracy so far: 0.948\n",
            "Step: 200 Loss: 2.182588815689087\n",
            "Total running accuracy so far: 0.948\n",
            "Step: 250 Loss: 2.181264877319336\n",
            "Total running accuracy so far: 0.948\n",
            "Step: 0 Loss: 2.1804027557373047\n",
            "Total running accuracy so far: 0.960\n",
            "Step: 50 Loss: 2.1790034770965576\n",
            "Total running accuracy so far: 0.950\n",
            "Step: 100 Loss: 2.189915418624878\n",
            "Total running accuracy so far: 0.950\n",
            "Step: 150 Loss: 2.191950559616089\n",
            "Total running accuracy so far: 0.951\n",
            "Step: 200 Loss: 2.181250810623169\n",
            "Total running accuracy so far: 0.951\n",
            "Step: 250 Loss: 2.180314779281616\n",
            "Total running accuracy so far: 0.951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_HmJ2cWEiuH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "9b56c237-fe35-48ca-8d61-cbcfab66b10f"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "import numpy as np\n",
        "fig = plt.figure()\n",
        "ax1 = plt.axes()\n",
        "ax1.plot(loss_total);\n",
        "plt.title(\"Total Loss\")\n",
        "\n",
        "fig = plt.figure()\n",
        "ax2 = plt.axes()\n",
        "ax2.plot(loss_total_layer);\n",
        "plt.title(\"Layer Loss\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Layer Loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsg8cOE7jxol",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "38e0ae2b-fa03-4888-ecee-f8a9a111f0f2"
      },
      "source": [
        "print(mlp.weights[1].shape)\n",
        "for i in range(len(mlp.weights)): print(mlp.weights[i].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256,)\n",
            "(784, 256)\n",
            "(256,)\n",
            "(256,)\n",
            "(256,)\n",
            "(256, 10)\n",
            "(10,)\n",
            "(10,)\n",
            "(10,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6PcaHbxlx9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HpEb3eKiIQq",
        "colab_type": "text"
      },
      "source": [
        "# **Jacob Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhLnL4KjiRoi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "6807a68a-bec3-42a5-c24e-375f88fc4213"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "X_ = np.load('./x_train.npy')\n",
        "y_ = np.load('./y_train.npy')\n",
        "X = np.zeros([133800,10])\n",
        "y = np.zeros(133800)\n",
        "X[0:133678, : ] = X_\n",
        "y[0:133678] = y_\n",
        "X[133678:133800,:] = X_[0:122,:]\n",
        "y[133678:133800] = y_[0:122]\n",
        "#X = X_[0:133600,:]\n",
        "#y = y_[0:133600 ]\n",
        "print(X_.shape)\n",
        "print(y_.shape)\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(133678, 10)\n",
            "(133678,)\n",
            "(133800, 10)\n",
            "(133800,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iilcODg3i6Is",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#x_train = x_train.reshape(x_train.shape[0], -1) / 255\n",
        "#x_test = x_test.reshape(x_test.shape[0], -1) / 255\n",
        "\n",
        "batch_size = 200 # MUST HAVE ALL BATCHES of EQUAL SIZE (The last one may mess some )\n",
        "\n",
        "one_hot_y_train = tf.one_hot(y.astype(np.float32), depth=2)\n",
        "#one_hot_y_test = tf.one_hot(y_test.astype(np.float32), depth=10)\n",
        "\n",
        "tr_dataset = tf.data.Dataset.from_tensor_slices((X, one_hot_y_train)).batch(batch_size)\n",
        "#val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Vw5dI7ofpa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a514a477-a077-498d-b449-0443d8444499"
      },
      "source": [
        "tr_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((None, 10), (None, 2)), types: (tf.float64, tf.float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIe462MOjVwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def x_Sigma_w_x_T(x, W_Sigma):\n",
        "  batch_sz = x.shape[0]\n",
        "  xx_t = tf.reduce_sum(tf.multiply(x, x),axis=1, keepdims=True)               # xxT is being calcualted\n",
        "  xx_t_e = tf.expand_dims(xx_t,axis=2)                                      # Expand dimention of xxt\n",
        "  return tf.multiply(xx_t_e, W_Sigma)\n",
        "\n",
        "def w_t_Sigma_i_w (w_mu, in_Sigma):\n",
        "  Sigma_1_1 = tf.matmul(tf.transpose(w_mu), in_Sigma)\n",
        "  Sigma_1_2 = tf.matmul(Sigma_1_1, w_mu)\n",
        "  return Sigma_1_2\n",
        "\n",
        "def tr_Sigma_w_Sigma_in (in_Sigma, W_Sigma):\n",
        "  Sigma_3_1 = tf.linalg.trace(in_Sigma)\n",
        "  Sigma_3_2 = tf.expand_dims(Sigma_3_1, axis=1)\n",
        "  Sigma_3_3 = tf.expand_dims(Sigma_3_2, axis=1)\n",
        "  return tf.multiply(Sigma_3_3, W_Sigma) \n",
        "\n",
        "def activation_Sigma (gradi, Sigma_in):\n",
        "  grad1 = tf.expand_dims(gradi,axis=2)\n",
        "  grad2 = tf.expand_dims(gradi,axis=1)\n",
        "  return tf.multiply(Sigma_in, tf.matmul(grad1, grad2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRHwWYS7jd8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Linear Class - First Layer (Constant * RV)\n",
        "class LinearFirst(keras.layers.Layer):\n",
        "    \"\"\"y = w.x + b\"\"\"\n",
        "\n",
        "    def __init__(self, units):\n",
        "        super(LinearFirst, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w_mu = self.add_weight(name='w_mu',\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None),\n",
        "            trainable=True\n",
        "        )\n",
        "        self.w_sigma = self.add_weight(name='w_sigma',\n",
        "            shape=(self.units,),\n",
        "            initializer=tf.random_uniform_initializer(minval= -12., maxval=-2.2, seed=None),\n",
        "            trainable=True\n",
        "        )\n",
        "        self.b_mu = self.add_weight(name='b_mu',\n",
        "            shape=(self.units,), initializer=tf.random_normal_initializer( mean=0.0, stddev=0.00005, seed=None),\n",
        "            trainable=True\n",
        "        )\n",
        "        self.b_sigma = self.add_weight(name='b_sigma',\n",
        "            shape=(self.units,), initializer=tf.random_uniform_initializer(minval= -12., maxval=-10., seed=None),\n",
        "            trainable=True\n",
        "        )\n",
        "    def call(self, inputs):\n",
        "        # Mean\n",
        "        #print(self.w_mu.shape)\n",
        "        mu_out = tf.matmul(inputs, self.w_mu) + self.b_mu                         # Mean of the output\n",
        "        # Varinace\n",
        "        W_Sigma = tf.linalg.diag(tf.math.log(1. + tf.math.exp(self.w_sigma)))                                    # Construct W_Sigma from w_sigmas\n",
        "        #W_Sigma = tf.linalg.diag(self.w_sigma)                                    # Construct W_Sigma from w_sigmas\n",
        "        Sigma_out = x_Sigma_w_x_T(inputs, W_Sigma) + tf.math.log(1. + tf.math.exp(self.b_sigma)) #tf.linalg.diag(self.b_sigma)\n",
        "\n",
        "        Term1 = tf.reduce_mean(tf.math.log(1.0 + Sigma_out))\n",
        "        Term2 = tf.reduce_mean(tf.square(mu_out))\n",
        "        Term3 = tf.reduce_mean(Sigma_out)\n",
        "      \n",
        "        kl_loss = -0.5 * (Term1 - Term2 - Term3)\n",
        "        self.add_loss(kl_loss)\n",
        "\n",
        "        return mu_out, Sigma_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06VIiaOijnmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Linear Class - Second Layer (RV * RV)\n",
        "class LinearNotFirst(keras.layers.Layer):\n",
        "    \"\"\"y = w.x + b\"\"\"\n",
        "\n",
        "    def __init__(self, units):\n",
        "        super(LinearNotFirst, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w_mu = self.add_weight(name='w_mu',\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=tf.random_normal_initializer( mean=0.0, stddev=0.05, seed=None),\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.w_sigma = self.add_weight(name='w_sigma',\n",
        "            shape=(self.units,),\n",
        "            initializer=tf.random_uniform_initializer(minval= -12., maxval=-2.2, seed=None),\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b_mu = self.add_weight(name='b_mu',\n",
        "            shape=(self.units,), initializer=tf.random_normal_initializer( mean=0.0, stddev=0.00005, seed=None),\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b_sigma = self.add_weight(name='b_sigma',\n",
        "            shape=(self.units,), initializer=tf.random_uniform_initializer(minval= -12., maxval=-10., seed=None),\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def call(self, mu_in, Sigma_in):\n",
        "        #batch_sz = mu_in.shape[0]\n",
        "        mu_out = tf.matmul(mu_in, self.w_mu) + self.b_mu\n",
        "        W_Sigma = tf.linalg.diag(tf.math.log(1. + tf.math.exp(self.w_sigma)))  \n",
        "        #W_Sigma = tf.linalg.diag(self.w_sigma)                                    # Construct W_Sigma from w_sigmas\n",
        "        # Simga_out has three terms\n",
        "        Sigma_1 = w_t_Sigma_i_w (self.w_mu, Sigma_in)\n",
        "        Sigma_2 = x_Sigma_w_x_T(mu_in, W_Sigma)                                   #tf.keras.backend.print_tensor(inputs2[0,:,:])\n",
        "        Sigma_3 = tr_Sigma_w_Sigma_in (Sigma_in, W_Sigma)\n",
        "        Sigma_out = Sigma_1 + Sigma_2 + Sigma_3 +tf.linalg.diag(tf.math.log(1. + tf.math.exp(self.b_sigma)))  #+ tf.linalg.diag(self.b_sigma)\n",
        "        \n",
        "        Term1 = tf.reduce_mean(tf.math.log(1.0 + Sigma_out))\n",
        "        Term2 = tf.reduce_mean(tf.square(mu_out))\n",
        "        Term3 = tf.reduce_mean(Sigma_out)\n",
        "        kl_loss = -0.5 * (Term1 - Term2 - Term3)\n",
        "        self.add_loss(kl_loss)\n",
        "        return mu_out, Sigma_out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhsXa_BnjvD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myReLU(keras.layers.Layer):\n",
        "    \"\"\"ReLU\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(myReLU, self).__init__()\n",
        "    def call(self, mu_in, Sigma_in):\n",
        "        mu_out = tf.nn.relu(mu_in)\n",
        "        with tf.GradientTape() as g:\n",
        "          g.watch(mu_in)\n",
        "          out = tf.nn.relu(mu_in)\n",
        "        gradi = g.gradient(out, mu_in) \n",
        "\n",
        "        Sigma_out = activation_Sigma (gradi, Sigma_in)\n",
        "        \n",
        "        return mu_out, Sigma_out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk5vaixsjwAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class mysoftmax(keras.layers.Layer):\n",
        "    \"\"\"Mysoftmax\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(mysoftmax, self).__init__()\n",
        "    def call(self, mu_in, Sigma_in):\n",
        "        mu_out = tf.nn.softmax(mu_in)\n",
        "        pp1 = tf.expand_dims(mu_out, axis=2)\n",
        "        pp2 = tf.expand_dims(mu_out, axis=1)\n",
        "        ppT = tf.matmul(pp1, pp2)\n",
        "        p_diag = tf.linalg.diag(mu_out)\n",
        "        grad = p_diag - ppT\n",
        "        Sigma_out = tf.matmul(grad, tf.matmul(Sigma_in, tf.transpose(grad, perm=[0, 2, 1])))\n",
        "        return mu_out, Sigma_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVxmYal_j77i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #############################\n",
        "###### Cutom training loop\n",
        "class exVDPMLP(tf.keras.Model):\n",
        "    \"\"\"Stack of Linear layers with a KL regularization loss.\"\"\"\n",
        "\n",
        "    def __init__(self, name=None):\n",
        "        super(exVDPMLP, self).__init__()\n",
        "        self.linear_1 = LinearFirst(256)\n",
        "        self.myrelu_1 = myReLU()\n",
        "        self.linear_2 = LinearNotFirst(2)\n",
        "        self.mysoftma = mysoftmax()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        m, s = self.linear_1 (inputs)\n",
        "        m, s = self.myrelu_1 (m, s)\n",
        "        m, s = self.linear_2 (m, s)\n",
        "        outputs, Sigma = self.mysoftma(m, s)\n",
        "        #print('Sample Sigma', Sigma[0,:,:])\n",
        "        return outputs, Sigma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktw8WctWkI2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nll_gaussian(y_test, y_pred_mean, y_pred_sd, num_labels=2, batch_size=200):\n",
        "    NS = tf.linalg.diag(tf.constant(1e-3, shape=[batch_size, num_labels]))\n",
        "    I = tf.eye(num_labels, batch_shape=[batch_size])\n",
        "    y_pred_sd_ns = y_pred_sd + NS\n",
        "    y_pred_sd_inv = tf.linalg.solve(y_pred_sd_ns, I)\n",
        "    mu_ = y_pred_mean - y_test\n",
        "   # return tf.reduce_mean(mu_**2)\n",
        "    mu_sigma = tf.matmul(mu_ ,  y_pred_sd_inv) \n",
        "    ms = 0.5*tf.matmul(mu_sigma , mu_, transpose_b=True) + 0.5*tf.linalg.slogdet(y_pred_sd_ns)[1]\n",
        "    \n",
        "    ms = tf.reduce_mean(ms)\n",
        "    return(ms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCBKmrW4kQNT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "3f4e5bae-d6a5-4d6c-c3ee-af9af6e1eeeb"
      },
      "source": [
        "# Cutom Trianing Loop with Graph\n",
        "mlp = exVDPMLP(name='mlp')\n",
        "#accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "#loss_fn = NLL_loss(num_labels=10, batch_size=100)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_total=[]\n",
        "@tf.function  # Make it fast.\n",
        "def train_on_batch(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits, sigma = mlp(x)\n",
        "        \n",
        "       # loss_final = loss_fn( y, logits, sigma)\n",
        "        loss_final = nll_gaussian(y, logits,   tf.clip_by_value(t=sigma, clip_value_min=tf.constant(-1e+10),\n",
        "                                  clip_value_max=tf.constant(1e+10)))\n",
        "        #loss_layers = sum(gru_model.losses)\n",
        "\n",
        "        loss = loss_final #+ loss_layers\n",
        "\n",
        "        gradients = tape.gradient(loss, mlp.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n",
        "    #acc = accuracy.update_state(y, logits)\n",
        "    return loss, logits\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "  print('Epoch: ', epoch, '/' , epochs)\n",
        "  for step, (x, y) in enumerate(tr_dataset):\n",
        "    loss, logits = train_on_batch(x, y)\n",
        "   # logits1 = tf.math.argmax( logits, axis=1)\n",
        "   # y1 = tf.math.argmax( y, axis=1)\n",
        "\n",
        "    corr = tf.equal(tf.math.argmax(logits, 1),tf.math.argmax(y,1))   \n",
        "    accuracy = tf.reduce_mean(tf.cast(corr,tf.float32))\n",
        "\n",
        "  #  print(logits1)\n",
        "   # print(y1)\n",
        "   # accuracy.update_state(y1, logits1)\n",
        "    if step % 1000 == 0:\n",
        "        print(\"Step:\", step, \"Loss:\", float(loss))\n",
        "        print(\"Total running accuracy so far: %.3f\" % accuracy)\n",
        "        loss_total.append(loss)\n",
        "  #for step, (x, y) in enumerate(val_dataset):\n",
        "  #  logits, sigma = mlp(x)\n",
        "  #  corr = tf.equal(tf.math.argmax(logits, 1),tf.math.argmax(y,1))   \n",
        "  #  va_accuracy = tf.reduce_mean(tf.cast(corr,tf.float32))\n",
        "  #  if step % 1 == 0:\n",
        "  #    print(\"Total running accuracy so far: %.3f\" % va_accuracy)\n",
        "       # loss_total_layer.append(loss_layers)\n",
        "  #accuracy.reset_states()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0 / 2\n",
            "WARNING:tensorflow:Layer ex_vdpmlp_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Step: 0 Loss: 64.74752044677734\n",
            "Total running accuracy so far: 0.240\n",
            "Epoch:  1 / 2\n",
            "Step: 0 Loss: -4.476774215698242\n",
            "Total running accuracy so far: 0.985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2-lyl17vgjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp.save_weights('./mlp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cluA19RxtKIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp.load_weights('./mlp')\n",
        "test_no_steps = 0\n",
        "logits_ = np.zeros([int(X.shape[0] / (batch_size)), batch_size, 2])\n",
        "sigma_ = np.zeros([int(X.shape[0] / (batch_size)), batch_size, 2, 2])\n",
        "for step, (x, y) in enumerate(tr_dataset):\n",
        "  logits, sigma = mlp(x)  \n",
        "  logits_[test_no_steps,:,:] =logits\n",
        "  sigma_[test_no_steps, :, :, :]= sigma\n",
        "  test_no_steps+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzwQ6Ln6wtyS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "d662f011-12ce-4922-f1eb-c92a46624bb7"
      },
      "source": [
        "sigma_[0,0,:,:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3.66357781e-05, -3.66358508e-05],\n",
              "       [-3.66358508e-05,  3.66359309e-05]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aPA3_JYQpqn",
        "colab_type": "text"
      },
      "source": [
        "**MLP_Jacob_Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQdZOhcD8O3g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "0d79b435-7cef-438c-b5ed-290eabd07e96"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import imageio\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import time, sys\n",
        "import pickle\n",
        "import timeit\n",
        "#from Adding_noise import random_noise\n",
        "\n",
        "plt.ioff()\n",
        "\n",
        "# update_progress() : Displays or updates a console progress bar\n",
        "## Accepts a float between 0 and 1. Any int will be converted to a float.\n",
        "## A value under 0 represents a 'halt'.\n",
        "## A value at 1 or bigger represents 100%\n",
        "def update_progress(progress):\n",
        "    barLength = 10 # Modify this to change the length of the progress bar\n",
        "    status = \"\"\n",
        "    if isinstance(progress, int):\n",
        "        progress = float(progress)\n",
        "    if not isinstance(progress, float):\n",
        "        progress = 0\n",
        "        status = \"error: progress var must be float\\r\\n\"\n",
        "    if progress < 0:\n",
        "        progress = 0\n",
        "        status = \"Halt...\\r\\n\"\n",
        "    if progress >= 1:\n",
        "        progress = 1\n",
        "        status = \"Done...\\r\\n\"\n",
        "    block = int(round(barLength*progress))\n",
        "    text = \"\\rPercent: [{0}] {1}% {2}\".format( \"#\"*block + \"-\"*(barLength-block), progress*100, status)\n",
        "    sys.stdout.write(text)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def x_Sigma_w_x_T(x, W_Sigma):\n",
        "  batch_sz = x.shape[0]\n",
        "  xx_t = tf.reduce_sum(tf.multiply(x, x),axis=1, keepdims=True)               # xxT is being calcualted\n",
        "  xx_t_e = tf.expand_dims(xx_t,axis=2)                                      # Expand dimention of xxt\n",
        "  return tf.multiply(xx_t_e, W_Sigma)\n",
        "\n",
        "def w_t_Sigma_i_w (w_mu, in_Sigma):\n",
        "  Sigma_1_1 = tf.matmul(tf.transpose(w_mu), in_Sigma)\n",
        "  Sigma_1_2 = tf.matmul(Sigma_1_1, w_mu)\n",
        "  return Sigma_1_2\n",
        "\n",
        "def tr_Sigma_w_Sigma_in (in_Sigma, W_Sigma):\n",
        "  Sigma_3_1 = tf.linalg.trace(in_Sigma)\n",
        "  Sigma_3_2 = tf.expand_dims(Sigma_3_1, axis=1)\n",
        "  Sigma_3_3 = tf.expand_dims(Sigma_3_2, axis=1)\n",
        "  return tf.multiply(Sigma_3_3, W_Sigma) \n",
        "\n",
        "def activation_Sigma (gradi, Sigma_in):\n",
        "  grad1 = tf.expand_dims(gradi,axis=2)\n",
        "  grad2 = tf.expand_dims(gradi,axis=1)\n",
        "  return tf.multiply(Sigma_in, tf.matmul(grad1, grad2))\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "# Linear Class - First Layer (Constant * RV)\n",
        "class LinearFirst(keras.layers.Layer):\n",
        "    \"\"\"y = w.x + b\"\"\"\n",
        "\n",
        "    def __init__(self, units):\n",
        "        super(LinearFirst, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w_mu = self.add_weight(name='w_mu',\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None),\n",
        "            trainable=True\n",
        "        )\n",
        "        self.w_sigma = self.add_weight(name='w_sigma',\n",
        "            shape=(self.units,),\n",
        "            initializer=tf.random_uniform_initializer(minval= -12., maxval=-2.2, seed=None),\n",
        "            trainable=True\n",
        "        )\n",
        "        self.b_mu = self.add_weight(name='b_mu',\n",
        "            shape=(self.units,), initializer=tf.random_normal_initializer( mean=0.0, stddev=0.00005, seed=None),\n",
        "            trainable=True\n",
        "        )\n",
        "        self.b_sigma = self.add_weight(name='b_sigma',\n",
        "            shape=(self.units,), initializer=tf.random_uniform_initializer(minval= -12., maxval=-10., seed=None),\n",
        "            trainable=True\n",
        "        )\n",
        "    def call(self, inputs):\n",
        "        # Mean\n",
        "        #print(self.w_mu.shape)\n",
        "        mu_out = tf.matmul(inputs, self.w_mu) + self.b_mu                         # Mean of the output\n",
        "        # Varinace\n",
        "        W_Sigma = tf.linalg.diag(tf.math.log(1. + tf.math.exp(self.w_sigma)))                                    # Construct W_Sigma from w_sigmas\n",
        "        #W_Sigma = tf.linalg.diag(self.w_sigma)                                    # Construct W_Sigma from w_sigmas\n",
        "        Sigma_out = x_Sigma_w_x_T(inputs, W_Sigma) + tf.math.log(1. + tf.math.exp(self.b_sigma)) #tf.linalg.diag(self.b_sigma)\n",
        "\n",
        "        Term1 = tf.reduce_mean(tf.math.log(1.0 + Sigma_out))\n",
        "        Term2 = tf.reduce_mean(tf.square(mu_out))\n",
        "        Term3 = tf.reduce_mean(Sigma_out)\n",
        "      \n",
        "        kl_loss = -0.5 * (Term1 - Term2 - Term3)\n",
        "        self.add_loss(kl_loss)\n",
        "\n",
        "        return mu_out, Sigma_out\n",
        "\n",
        "\n",
        "# Linear Class - Second Layer (RV * RV)\n",
        "class LinearNotFirst(keras.layers.Layer):\n",
        "    \"\"\"y = w.x + b\"\"\"\n",
        "\n",
        "    def __init__(self, units):\n",
        "        super(LinearNotFirst, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w_mu = self.add_weight(name='w_mu',\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=tf.random_normal_initializer( mean=0.0, stddev=0.05, seed=None),\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.w_sigma = self.add_weight(name='w_sigma',\n",
        "            shape=(self.units,),\n",
        "            initializer=tf.random_uniform_initializer(minval= -12., maxval=-2.2, seed=None),\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b_mu = self.add_weight(name='b_mu',\n",
        "            shape=(self.units,), initializer=tf.random_normal_initializer( mean=0.0, stddev=0.00005, seed=None),\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b_sigma = self.add_weight(name='b_sigma',\n",
        "            shape=(self.units,), initializer=tf.random_uniform_initializer(minval= -12., maxval=-10., seed=None),\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def call(self, mu_in, Sigma_in):\n",
        "        #batch_sz = mu_in.shape[0]\n",
        "        mu_out = tf.matmul(mu_in, self.w_mu) + self.b_mu\n",
        "        \n",
        "      #  W_Sigma = tf.linalg.diag(self.w_sigma)  \n",
        "        W_Sigma = tf.linalg.diag(tf.math.log(1. + tf.math.exp(self.w_sigma)))                                   # Construct W_Sigma from w_sigmas\n",
        "        # Simga_out has three terms\n",
        "        Sigma_1 = w_t_Sigma_i_w (self.w_mu, Sigma_in)\n",
        "        Sigma_2 = x_Sigma_w_x_T(mu_in, W_Sigma)                                   \n",
        "        Sigma_3 = tr_Sigma_w_Sigma_in (Sigma_in, W_Sigma)\n",
        "        Sigma_out = Sigma_1 + Sigma_2 + Sigma_3 + tf.linalg.diag(tf.math.log(1. + tf.math.exp(self.b_sigma))) #+ tf.linalg.diag(self.b_sigma)\n",
        "        \n",
        "        Term1 = tf.reduce_mean(tf.math.log(1.0 + Sigma_out))\n",
        "        Term2 = tf.reduce_mean(tf.square(mu_out))\n",
        "        Term3 = tf.reduce_mean(Sigma_out)\n",
        "        kl_loss = -0.5 * (Term1 - Term2 - Term3)\n",
        "        self.add_loss(kl_loss)\n",
        "        return mu_out, Sigma_out\n",
        "\n",
        "class myReLU(keras.layers.Layer):\n",
        "    \"\"\"ReLU\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(myReLU, self).__init__()\n",
        "    def call(self, mu_in, Sigma_in):\n",
        "        mu_out = tf.nn.relu(mu_in)\n",
        "        with tf.GradientTape() as g:\n",
        "          g.watch(mu_in)\n",
        "          out = tf.nn.relu(mu_in)\n",
        "        gradi = g.gradient(out, mu_in) \n",
        "\n",
        "        Sigma_out = activation_Sigma (gradi, Sigma_in)\n",
        "        \n",
        "        return mu_out, Sigma_out\n",
        "\n",
        "class mysoftmax(keras.layers.Layer):\n",
        "    \"\"\"Mysoftmax\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(mysoftmax, self).__init__()\n",
        "    def call(self, mu_in, Sigma_in):\n",
        "        mu_out = tf.nn.softmax(mu_in)\n",
        "        pp1 = tf.expand_dims(mu_out, axis=2)\n",
        "        pp2 = tf.expand_dims(mu_out, axis=1)\n",
        "        ppT = tf.matmul(pp1, pp2)\n",
        "        p_diag = tf.linalg.diag(mu_out)\n",
        "        grad = p_diag - ppT\n",
        "        Sigma_out = tf.matmul(grad, tf.matmul(Sigma_in, tf.transpose(grad, perm=[0, 2, 1])))\n",
        "        return mu_out, Sigma_out\n",
        "\n",
        "\n",
        "def nll_gaussian(y_test, y_pred_mean, y_pred_sd, num_labels=2, batch_size=200):\n",
        "    NS = tf.linalg.diag(tf.constant(1e-3, shape=[batch_size, num_labels]))\n",
        "    I = tf.eye(num_labels, batch_shape=[batch_size])\n",
        "    y_pred_sd_ns = y_pred_sd + NS\n",
        "    y_pred_sd_inv = tf.linalg.solve(y_pred_sd_ns, I)\n",
        "    mu_ = y_pred_mean - y_test\n",
        "    mu_sigma = tf.matmul(mu_ ,  y_pred_sd_inv) \n",
        "    ms = 0.5*tf.matmul(mu_sigma , mu_, transpose_b=True) + 0.5*tf.linalg.slogdet(y_pred_sd_ns)[1]\n",
        "    ms = tf.reduce_mean(ms)\n",
        "    return(ms)\n",
        "\n",
        "class exVDPMLP(tf.keras.Model):\n",
        "    \"\"\"Stack of Linear layers with a KL regularization loss.\"\"\"\n",
        "\n",
        "    def __init__(self, name=None):\n",
        "        super(exVDPMLP, self).__init__()\n",
        "        self.linear_1 = LinearFirst(256)\n",
        "        self.myrelu_1 = myReLU()\n",
        "        self.linear_2 = LinearNotFirst(2)\n",
        "        self.mysoftma = mysoftmax()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        m, s = self.linear_1 (inputs)\n",
        "        m, s = self.myrelu_1 (m, s)\n",
        "        m, s = self.linear_2 (m, s)\n",
        "        outputs, Sigma = self.mysoftma(m, s)\n",
        "        #print('Sample Sigma', Sigma[0,:,:])\n",
        "        return outputs, Sigma\n",
        "\n",
        "def main_function( input_dim = 10, units = 256, output_size = 2 , batch_size = 200, epochs = 5, lr = 0.001, \n",
        "         Random_noise=True, gaussain_noise_std=10000, Training = False):\n",
        "    \n",
        "\n",
        "    PATH = './saved_models/DP_MLP_epoch_{}/'.format(epochs)\n",
        "    X = np.load('./x_train.npy')\n",
        "    y = np.load('./y_train.npy')\n",
        "    x_train = X[0:120000,:]\n",
        "    y_train = y[0:120000]\n",
        "    x_test = X[120000:133600,:]\n",
        "    y_test = y[120000:133600]   \n",
        "\n",
        "    one_hot_y_train = tf.one_hot(y_train.astype(np.float32), depth=2)\n",
        "    one_hot_y_test = tf.one_hot(y_test.astype(np.float32), depth=2) \n",
        "\n",
        "    tr_dataset = tf.data.Dataset.from_tensor_slices((x_train, one_hot_y_train)).batch(batch_size)\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((x_test, one_hot_y_test)).batch(batch_size)\n",
        "\n",
        "        \n",
        "    # Cutom Trianing Loop with Graph\n",
        "    mlp_model = exVDPMLP(name='mlp')    \n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    \n",
        "    @tf.function  # Make it fast.\n",
        "    def train_on_batch(x, y):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits, sigma = mlp_model(x)      \n",
        "            loss_final = nll_gaussian(y, logits,  tf.clip_by_value(t=sigma, clip_value_min=tf.constant(-1e+10),\n",
        "                                       clip_value_max=tf.constant(1e+10)), output_size , batch_size)\n",
        "            #loss_layers = sum(mlp_model.losses)\n",
        "\n",
        "            loss = loss_final #+ loss_layers\n",
        "            gradients = tape.gradient(loss, mlp_model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(gradients, mlp_model.trainable_weights))        \n",
        "        return loss, logits\n",
        "    if Training:\n",
        "        train_acc = np.zeros(epochs) \n",
        "        valid_acc = np.zeros(epochs)\n",
        "        train_err = np.zeros(epochs)\n",
        "        valid_error = np.zeros(epochs)\n",
        "        start = timeit.default_timer()\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "          print('Epoch: ', epoch+1, '/' , epochs)\n",
        "    \n",
        "          acc1 = 0 \n",
        "          acc_valid1 = 0 \n",
        "          err1 = 0\n",
        "          err_valid1 = 0\n",
        "          tr_no_steps = 0          \n",
        "          #Training\n",
        "          for step, (x, y) in enumerate(tr_dataset):\n",
        "              update_progress(step / int(x_train.shape[0] / (batch_size)) )  \n",
        "              loss, logits = train_on_batch(x, y)\n",
        "              err1+= loss\n",
        "    \n",
        "              corr = tf.equal(tf.math.argmax(logits, axis=1),tf.math.argmax(y,axis=1))\n",
        "              accuracy = tf.reduce_mean(tf.cast(corr,tf.float32))\n",
        "              acc1+=accuracy\n",
        "            #  if step % 2000 == 0:\n",
        "            #      print(\"Step:\", step, \"Loss:\", float(loss))\n",
        "            #      print(\"Total running accuracy so far: %.3f\" % accuracy)             \n",
        "                  \n",
        "              tr_no_steps+=1\n",
        "          train_acc[epoch] = acc1/tr_no_steps\n",
        "          train_err[epoch] = err1/tr_no_steps\n",
        "          \n",
        "          print('Training Acc  ', train_acc[epoch])\n",
        "          print('Training error  ', train_err[epoch])          \n",
        "          \n",
        "        stop = timeit.default_timer()\n",
        "        print('Total Training Time: ', stop - start)\n",
        "        print('Training Acc  ', np.mean(train_acc))          \n",
        "        print('Training error  ', np.mean(train_err))       \n",
        "    \n",
        "        \n",
        "        mlp_model.save_weights(PATH + 'DP_MLP_model')\n",
        "        \n",
        "        if (epochs > 1):\n",
        "            fig = plt.figure(figsize=(15,7))\n",
        "            plt.plot(train_acc, 'b', label='Training acc')            \n",
        "            plt.ylim(0, 1.1)\n",
        "            plt.title(\"Density Propagation MLP on MNIST Data\")\n",
        "            plt.xlabel(\"Epochs\")\n",
        "            plt.ylabel(\"Accuracy\")\n",
        "            plt.legend(loc='lower right')\n",
        "            plt.savefig(PATH + 'DP_MLP_on_MNIST_Data_acc.png')\n",
        "            plt.close(fig)\n",
        "    \n",
        "    \n",
        "            fig = plt.figure(figsize=(15,7))\n",
        "            plt.plot(train_err, 'b', label='Training error')            \n",
        "            plt.title(\"Density Propagation MLP on MNIST Data\")\n",
        "            plt.xlabel(\"Epochs\")\n",
        "            plt.ylabel(\"Error\")\n",
        "            plt.legend(loc='lower right')\n",
        "            plt.savefig(PATH + 'DP_MLP_on_MNIST_Data_error.png')\n",
        "            plt.close(fig)\n",
        "        f = open(PATH + 'training_validation_acc_error.pkl', 'wb')         \n",
        "        pickle.dump([train_acc, train_err], f)                                                   \n",
        "        f.close()         \n",
        "             \n",
        "             \n",
        "        textfile = open(PATH + 'Related_hyperparameters.txt','w')    \n",
        "        textfile.write(' Input Dimension : ' +str(input_dim))\n",
        "        textfile.write('\\n No Hidden Nodes : ' +str(units))\n",
        "        textfile.write('\\n Output Size : ' +str(output_size))\n",
        "        textfile.write('\\n No of epochs : ' +str(epochs))\n",
        "        textfile.write('\\n Learning rate : ' +str(lr))            \n",
        "        textfile.write(\"\\n---------------------------------\")\n",
        "        if Training: \n",
        "            textfile.write('\\n Total run time in sec : ' +str(stop - start))\n",
        "            if(epochs == 1):\n",
        "                textfile.write(\"\\n Averaged Training  Accuracy : \"+ str( train_acc))                  \n",
        "                textfile.write(\"\\n Averaged Training  error : \"+ str( train_err))               \n",
        "            else:\n",
        "                textfile.write(\"\\n Averaged Training  Accuracy : \"+ str(np.mean(train_acc)))               \n",
        "                textfile.write(\"\\n Averaged Training  error : \"+ str(np.mean(train_err)))               \n",
        "        textfile.write(\"\\n---------------------------------\")                \n",
        "        textfile.write(\"\\n---------------------------------\")    \n",
        "        textfile.close()\n",
        "        \n",
        "    else:\n",
        "        test_path = 'test_results_random_noise_{}/'.format(gaussain_noise_std)\n",
        "        mlp_model.load_weights(PATH + 'DP_MLP_model')\n",
        "        test_no_steps = 0\n",
        "        err_test = 0\n",
        "        acc_test = 0\n",
        "        true_x = np.zeros([int(x_test.shape[0] / (batch_size)), batch_size,  input_dim])\n",
        "        true_y = np.zeros([int(x_test.shape[0] / (batch_size)), batch_size, output_size])\n",
        "        logits_ = np.zeros([int(x_test.shape[0] / (batch_size)), batch_size, output_size])\n",
        "        sigma_ = np.zeros([int(x_test.shape[0] / (batch_size)), batch_size, output_size, output_size])\n",
        "        for step, (x, y) in enumerate(val_dataset):\n",
        "          update_progress(step / int(x_test.shape[0] / (batch_size)) ) \n",
        "          true_x[test_no_steps, :, :] = x\n",
        "          true_y[test_no_steps, :, :] = y\n",
        "          if Random_noise:\n",
        "              noise = tf.random.normal(shape = [batch_size, input_dim], mean = 0.0, stddev = gaussain_noise_std, dtype = x.dtype ) \n",
        "              x = x +  noise \n",
        "        \n",
        "          logits, sigma = mlp_model(x)  \n",
        "          logits_[test_no_steps,:,:] =logits\n",
        "          sigma_[test_no_steps, :, :, :]= sigma\n",
        "          tloss = nll_gaussian(y, logits,  tf.clip_by_value(t=sigma, clip_value_min=tf.constant(-1e+10), clip_value_max=tf.constant(1e+10)), output_size , batch_size)\n",
        "          err_test+= tloss\n",
        "          \n",
        "          corr = tf.equal(tf.math.argmax(logits, axis=1),tf.math.argmax(y,axis=1))\n",
        "          accuracy = tf.reduce_mean(tf.cast(corr,tf.float32))\n",
        "          acc_test+=accuracy\n",
        "\n",
        "          if step % 500 == 0:\n",
        "              print(\"Step:\", step, \"Loss:\", float(tloss))\n",
        "              print(\"Total running accuracy so far: %.3f\" % accuracy)              \n",
        "           # loss_total_layer.append(loss_layers)\n",
        "          test_no_steps+=1\n",
        "       \n",
        "        test_acc = acc_test/test_no_steps      \n",
        "        test_error = err_test/test_no_steps\n",
        "        print('Test accuracy : ', test_acc.numpy())\n",
        "        print('Test error : ', test_error.numpy())\n",
        "        \n",
        "        pf = open(PATH + test_path + 'uncertainty_info.pkl', 'wb')         \n",
        "        pickle.dump([logits_, sigma_, true_x, true_y, test_acc.numpy(), test_error.numpy()  ], pf)                                                   \n",
        "        pf.close()\n",
        "        \n",
        "        textfile = open(PATH + test_path + 'Related_hyperparameters.txt','w')    \n",
        "        textfile.write(' Input Dimension : ' +str(input_dim))\n",
        "        textfile.write('\\n No Hidden Nodes : ' +str(units))\n",
        "        textfile.write('\\n Output Size : ' +str(output_size))\n",
        "        textfile.write('\\n No of epochs : ' +str(epochs))\n",
        "        textfile.write('\\n Learning rate : ' +str(lr))          \n",
        "        textfile.write(\"\\n---------------------------------\")\n",
        "        textfile.write(\"\\n Averaged Test Accuracy : \"+ str( test_acc.numpy()))\n",
        "        textfile.write(\"\\n Averaged Test error : \"+ str(test_error.numpy() ))            \n",
        "        textfile.write(\"\\n---------------------------------\")\n",
        "        if Random_noise:\n",
        "            textfile.write('\\n Random Noise std: '+ str(gaussain_noise_std ))              \n",
        "        textfile.write(\"\\n---------------------------------\")    \n",
        "        textfile.close()\n",
        "            \n",
        "if __name__ == '__main__':\n",
        "    main_function()    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rPercent: [----------] 0.0% WARNING:tensorflow:Layer ex_vdpmlp_33 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Step: 0 Loss: -3.3077564239501953\n",
            "Total running accuracy so far: 0.920\n",
            "Percent: [##########] 98.52941176470588% Test accuracy :  0.92058825\n",
            "Test error :  -2.5596993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHwIU3Lw_a7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "outputId": "6843c3e0-721b-49c6-a048-13f8613a333e"
      },
      "source": [
        "#@title save yo data to drive\n",
        "filename = \"saved_models\" #@param {type:\"string\"}\n",
        "folders_or_files_to_save = \"saved_models\" #@param {type:\"string\"}\n",
        "from google.colab import files\n",
        "from google.colab import auth\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "def save_file_to_drive(name, path):\n",
        "    file_metadata = {\n",
        "    'name': name,\n",
        "    'mimeType': 'application/octet-stream'\n",
        "    }\n",
        "\n",
        "    media = MediaFileUpload(path, \n",
        "                  mimetype='application/octet-stream',\n",
        "                  resumable=True)\n",
        "\n",
        "    created = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
        "\n",
        "    print('File ID: {}'.format(created.get('id')))\n",
        "\n",
        "    return created\n",
        "\n",
        "\n",
        "extension_zip = \".zip\"\n",
        "\n",
        "zip_file = filename + extension_zip\n",
        "\n",
        "# !rm -rf $zip_file\n",
        "!zip -r $zip_file {folders_or_files_to_save} # FOLDERS TO SAVE INTO ZIP FILE\n",
        "\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "destination_name = zip_file\n",
        "path_to_file = zip_file\n",
        "save_file_to_drive(destination_name, path_to_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: saved_models/ (stored 0%)\n",
            "updating: saved_models/DP_MLP_epoch_1/ (stored 0%)\n",
            "updating: saved_models/DP_MLP_epoch_1/DP_MLP_model.data-00000-of-00002 (deflated 73%)\n",
            "updating: saved_models/DP_MLP_epoch_1/training_validation_acc_error.pkl (deflated 15%)\n",
            "updating: saved_models/DP_MLP_epoch_1/DP_MLP_model.data-00001-of-00002 (deflated 6%)\n",
            "updating: saved_models/DP_MLP_epoch_1/checkpoint (deflated 41%)\n",
            "updating: saved_models/DP_MLP_epoch_1/DP_MLP_model.index (deflated 54%)\n",
            "updating: saved_models/DP_MLP_epoch_1/Related_hyperparameters.txt (deflated 45%)\n",
            "updating: saved_models/.ipynb_checkpoints/ (stored 0%)\n",
            "updating: saved_models/DP_MLP_epoch_20/ (stored 0%)\n",
            "updating: saved_models/DP_MLP_epoch_20/DP_MLP_model.data-00000-of-00002 (deflated 73%)\n",
            "updating: saved_models/DP_MLP_epoch_20/training_validation_acc_error.pkl (deflated 45%)\n",
            "updating: saved_models/DP_MLP_epoch_20/DP_MLP_on_MNIST_Data_acc.png (deflated 26%)\n",
            "updating: saved_models/DP_MLP_epoch_20/test_results_random_noise_0.4/ (stored 0%)\n",
            "updating: saved_models/DP_MLP_epoch_20/test_results_random_noise_0.4/uncertainty_info.pkl (deflated 60%)\n",
            "updating: saved_models/DP_MLP_epoch_20/test_results_random_noise_0.4/Related_hyperparameters.txt (deflated 47%)\n",
            "updating: saved_models/DP_MLP_epoch_20/test_results_random_noise_0.1/ (stored 0%)\n",
            "updating: saved_models/DP_MLP_epoch_20/test_results_random_noise_0.1/uncertainty_info.pkl (deflated 59%)\n",
            "updating: saved_models/DP_MLP_epoch_20/test_results_random_noise_0.1/Related_hyperparameters.txt (deflated 47%)\n",
            "updating: saved_models/DP_MLP_epoch_20/test_results_random_noise_0.3/ (stored 0%)\n",
            "updating: saved_models/DP_MLP_epoch_20/test_results_random_noise_0.3/uncertainty_info.pkl (deflated 60%)\n",
            "updating: saved_models/DP_MLP_epoch_20/test_results_random_noise_0.3/Related_hyperparameters.txt (deflated 47%)\n",
            "updating: saved_models/DP_MLP_epoch_20/test_results_random_noise_1/ (stored 0%)\n",
            "updating: saved_models/DP_MLP_epoch_20/test_results_random_noise_1/uncertainty_info.pkl (deflated 60%)\n",
            "updating: saved_models/DP_MLP_epoch_20/test_results_random_noise_1/Related_hyperparameters.txt (deflated 47%)\n",
            "updating: saved_models/DP_MLP_epoch_20/DP_MLP_on_MNIST_Data_error.png (deflated 13%)\n",
            "updating: saved_models/DP_MLP_epoch_20/test_results_random_noise_0.5/ (stored 0%)\n",
            "updating: saved_models/DP_MLP_epoch_20/test_results_random_noise_0.5/uncertainty_info.pkl (deflated 60%)\n",
            "updating: saved_models/DP_MLP_epoch_20/test_results_random_noise_0.5/Related_hyperparameters.txt (deflated 47%)\n",
            "updating: saved_models/DP_MLP_epoch_20/DP_MLP_model.data-00001-of-00002 (deflated 4%)\n",
            "updating: saved_models/DP_MLP_epoch_20/.ipynb_checkpoints/ (stored 0%)\n",
            "updating: saved_models/DP_MLP_epoch_20/checkpoint (deflated 41%)\n",
            "updating: saved_models/DP_MLP_epoch_20/test_results_random_noise_0.2/ (stored 0%)\n",
            "updating: saved_models/DP_MLP_epoch_20/test_results_random_noise_0.2/uncertainty_info.pkl (deflated 60%)\n",
            "updating: saved_models/DP_MLP_epoch_20/test_results_random_noise_0.2/Related_hyperparameters.txt (deflated 47%)\n",
            "updating: saved_models/DP_MLP_epoch_20/DP_MLP_model.index (deflated 55%)\n",
            "updating: saved_models/DP_MLP_epoch_20/test_results_random_noise_0/ (stored 0%)\n",
            "updating: saved_models/DP_MLP_epoch_20/test_results_random_noise_0/uncertainty_info.pkl (deflated 60%)\n",
            "updating: saved_models/DP_MLP_epoch_20/test_results_random_noise_0/Related_hyperparameters.txt (deflated 47%)\n",
            "updating: saved_models/DP_MLP_epoch_20/Related_hyperparameters.txt (deflated 46%)\n",
            "updating: saved_models/DP_MLP_epoch_10/ (stored 0%)\n",
            "updating: saved_models/DP_MLP_epoch_10/DP_MLP_model.data-00000-of-00002 (deflated 73%)\n",
            "updating: saved_models/DP_MLP_epoch_10/training_validation_acc_error.pkl (deflated 35%)\n",
            "updating: saved_models/DP_MLP_epoch_10/DP_MLP_on_MNIST_Data_acc.png (deflated 27%)\n",
            "updating: saved_models/DP_MLP_epoch_10/DP_MLP_on_MNIST_Data_error.png (deflated 14%)\n",
            "updating: saved_models/DP_MLP_epoch_10/DP_MLP_model.data-00001-of-00002 (deflated 4%)\n",
            "updating: saved_models/DP_MLP_epoch_10/checkpoint (deflated 41%)\n",
            "updating: saved_models/DP_MLP_epoch_10/DP_MLP_model.index (deflated 54%)\n",
            "updating: saved_models/DP_MLP_epoch_10/Related_hyperparameters.txt (deflated 46%)\n",
            "File ID: 1aBPFxw4MmUHn3c6PZClfgyVvPYk8uv5b\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '1aBPFxw4MmUHn3c6PZClfgyVvPYk8uv5b'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A2mNc-2q9t9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "36ffc551-54b3-43a3-a967-79cda7de3403"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import imageio\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import time, sys\n",
        "import pickle\n",
        "import timeit\n",
        "#from Adding_noise import random_noise\n",
        "\n",
        "plt.ioff()\n",
        "\n",
        "# update_progress() : Displays or updates a console progress bar\n",
        "## Accepts a float between 0 and 1. Any int will be converted to a float.\n",
        "## A value under 0 represents a 'halt'.\n",
        "## A value at 1 or bigger represents 100%\n",
        "def update_progress(progress):\n",
        "    barLength = 10 # Modify this to change the length of the progress bar\n",
        "    status = \"\"\n",
        "    if isinstance(progress, int):\n",
        "        progress = float(progress)\n",
        "    if not isinstance(progress, float):\n",
        "        progress = 0\n",
        "        status = \"error: progress var must be float\\r\\n\"\n",
        "    if progress < 0:\n",
        "        progress = 0\n",
        "        status = \"Halt...\\r\\n\"\n",
        "    if progress >= 1:\n",
        "        progress = 1\n",
        "        status = \"Done...\\r\\n\"\n",
        "    block = int(round(barLength*progress))\n",
        "    text = \"\\rPercent: [{0}] {1}% {2}\".format( \"#\"*block + \"-\"*(barLength-block), progress*100, status)\n",
        "    sys.stdout.write(text)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def x_Sigma_w_x_T(x, W_Sigma):\n",
        "  batch_sz = x.shape[0]\n",
        "  xx_t = tf.reduce_sum(tf.multiply(x, x),axis=1, keepdims=True)               \n",
        "  xx_t_e = tf.expand_dims(xx_t,axis=2)                                     \n",
        "  return tf.multiply(xx_t_e, W_Sigma)\n",
        "\n",
        "def w_t_Sigma_i_w (w_mu, in_Sigma):\n",
        "  Sigma_1_1 = tf.matmul(tf.transpose(w_mu), in_Sigma)\n",
        "  Sigma_1_2 = tf.matmul(Sigma_1_1, w_mu)\n",
        "  return Sigma_1_2\n",
        "\n",
        "def tr_Sigma_w_Sigma_in (in_Sigma, W_Sigma):\n",
        "  Sigma_3_1 = tf.linalg.trace(in_Sigma)\n",
        "  Sigma_3_2 = tf.expand_dims(Sigma_3_1, axis=1)\n",
        "  Sigma_3_3 = tf.expand_dims(Sigma_3_2, axis=1)\n",
        "  return tf.multiply(Sigma_3_3, W_Sigma) \n",
        "\n",
        "def activation_Sigma (gradi, Sigma_in):\n",
        "  grad1 = tf.expand_dims(gradi,axis=2)\n",
        "  grad2 = tf.expand_dims(gradi,axis=1)\n",
        "  return tf.multiply(Sigma_in, tf.matmul(grad1, grad2))\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "# Linear Class - First Layer (Constant * RV)\n",
        "class LinearFirst(keras.layers.Layer):\n",
        "    \"\"\"y = w.x + b\"\"\"\n",
        "\n",
        "    def __init__(self, units):\n",
        "        super(LinearFirst, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w_mu = self.add_weight(name='w_mu',\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None),\n",
        "            trainable=True\n",
        "        )\n",
        "        self.w_sigma = self.add_weight(name='w_sigma',\n",
        "            shape=(self.units,),\n",
        "            initializer=tf.random_uniform_initializer(minval= -12., maxval=-2.2, seed=None),\n",
        "            trainable=True\n",
        "        )\n",
        "        self.b_mu = self.add_weight(name='b_mu',\n",
        "            shape=(self.units,), initializer=tf.random_normal_initializer( mean=0.0, stddev=0.00005, seed=None),\n",
        "            trainable=True\n",
        "        )\n",
        "        self.b_sigma = self.add_weight(name='b_sigma',\n",
        "            shape=(self.units,), initializer=tf.random_uniform_initializer(minval= -12., maxval=-10., seed=None),\n",
        "            trainable=True\n",
        "        )\n",
        "    def call(self, inputs):\n",
        "        # Mean\n",
        "        #print(self.w_mu.shape)\n",
        "        mu_out = tf.matmul(inputs, self.w_mu) + self.b_mu                         # Mean of the output\n",
        "        # Varinace\n",
        "        W_Sigma = tf.linalg.diag(tf.math.log(1. + tf.math.exp(self.w_sigma)))                                        \n",
        "        Sigma_out = x_Sigma_w_x_T(inputs, W_Sigma) + tf.math.log(1. + tf.math.exp(self.b_sigma)) \n",
        "\n",
        "        Term1 = self.w_mu.shape[0]*tf.math.log(tf.math.log(1. + tf.math.exp(self.w_sigma)))\n",
        "        Term2 = tf.reduce_sum(tf.reduce_sum(tf.abs(self.w_mu)))\n",
        "        Term3 = self.w_mu.shape[0]*tf.math.log(1. + tf.math.exp(self.w_sigma))      \n",
        "      \n",
        "        kl_loss = -0.5 * tf.reduce_mean(Term1 - Term2 - Term3)\n",
        "        self.add_loss(kl_loss)\n",
        "\n",
        "        return mu_out, Sigma_out\n",
        "\n",
        "\n",
        "# Linear Class - Second Layer (RV * RV)\n",
        "class LinearNotFirst(keras.layers.Layer):\n",
        "    \"\"\"y = w.x + b\"\"\"\n",
        "\n",
        "    def __init__(self, units):\n",
        "        super(LinearNotFirst, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w_mu = self.add_weight(name='w_mu',\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=tf.random_normal_initializer( mean=0.0, stddev=0.05, seed=None),\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.w_sigma = self.add_weight(name='w_sigma',\n",
        "            shape=(self.units,),\n",
        "            initializer=tf.random_uniform_initializer(minval= -12., maxval=-2.2, seed=None),\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b_mu = self.add_weight(name='b_mu',\n",
        "            shape=(self.units,), initializer=tf.random_normal_initializer( mean=0.0, stddev=0.00005, seed=None),\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b_sigma = self.add_weight(name='b_sigma',\n",
        "            shape=(self.units,), initializer=tf.random_uniform_initializer(minval= -12., maxval=-10., seed=None),\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def call(self, mu_in, Sigma_in):\n",
        "        \n",
        "        mu_out = tf.matmul(mu_in, self.w_mu) + self.b_mu    \n",
        "        W_Sigma = tf.linalg.diag(tf.math.log(1. + tf.math.exp(self.w_sigma)))                                   \n",
        "        # Simga_out has three terms\n",
        "        Sigma_1 = w_t_Sigma_i_w (self.w_mu, Sigma_in)\n",
        "        Sigma_2 = x_Sigma_w_x_T(mu_in, W_Sigma)                                   \n",
        "        Sigma_3 = tr_Sigma_w_Sigma_in (Sigma_in, W_Sigma)\n",
        "        Sigma_out = Sigma_1 + Sigma_2 + Sigma_3 + tf.linalg.diag(tf.math.log(1. + tf.math.exp(self.b_sigma))) \n",
        "        \n",
        "        Term1 = self.w_mu.shape[0]*tf.math.log(tf.math.log(1. + tf.math.exp(self.w_sigma)))\n",
        "        Term2 = tf.math.reduce_sum(tf.reduce_sum(tf.abs(self.w_mu)))\n",
        "        Term3 = self.w_mu.shape[0]*tf.math.log(1. + tf.math.exp(self.w_sigma))    \n",
        "        kl_loss = -0.5 * tf.reduce_mean(Term1 - Term2 - Term3)\n",
        "        self.add_loss(kl_loss)\n",
        "        return mu_out, Sigma_out\n",
        "\n",
        "class myReLU(keras.layers.Layer):\n",
        "    \"\"\"ReLU\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(myReLU, self).__init__()\n",
        "    def call(self, mu_in, Sigma_in):\n",
        "        mu_out = tf.nn.relu(mu_in)\n",
        "        with tf.GradientTape() as g:\n",
        "          g.watch(mu_in)\n",
        "          out = tf.nn.relu(mu_in)\n",
        "        gradi = g.gradient(out, mu_in) \n",
        "\n",
        "        Sigma_out = activation_Sigma (gradi, Sigma_in)\n",
        "        \n",
        "        return mu_out, Sigma_out\n",
        "\n",
        "class mysoftmax(keras.layers.Layer):\n",
        "    \"\"\"Mysoftmax\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(mysoftmax, self).__init__()\n",
        "    def call(self, mu_in, Sigma_in):\n",
        "        mu_out = tf.nn.softmax(mu_in)\n",
        "        pp1 = tf.expand_dims(mu_out, axis=2)\n",
        "        pp2 = tf.expand_dims(mu_out, axis=1)\n",
        "        ppT = tf.matmul(pp1, pp2)\n",
        "        p_diag = tf.linalg.diag(mu_out)\n",
        "        grad = p_diag - ppT\n",
        "        Sigma_out = tf.matmul(grad, tf.matmul(Sigma_in, tf.transpose(grad, perm=[0, 2, 1])))\n",
        "        return mu_out, Sigma_out\n",
        "\n",
        "\n",
        "def nll_gaussian(y_test, y_pred_mean, y_pred_sd, num_labels=2, batch_size=200):\n",
        "    NS = tf.linalg.diag(tf.constant(1e-3, shape=[batch_size, num_labels]))\n",
        "    I = tf.eye(num_labels, batch_shape=[batch_size])\n",
        "    y_pred_sd_ns = y_pred_sd + NS\n",
        "    y_pred_sd_inv = tf.linalg.solve(y_pred_sd_ns, I)\n",
        "    mu_ = y_pred_mean - y_test\n",
        "    mu_sigma = tf.matmul(mu_ ,  y_pred_sd_inv) \n",
        "    ms = 0.5*tf.matmul(mu_sigma , mu_, transpose_b=True) + 0.5*tf.linalg.slogdet(y_pred_sd_ns)[1]\n",
        "    ms = tf.reduce_mean(ms)\n",
        "    return(ms)\n",
        "\n",
        "class exVDPMLP(tf.keras.Model):\n",
        "    \"\"\"Stack of Linear layers with a KL regularization loss.\"\"\"\n",
        "\n",
        "    def __init__(self, name=None):\n",
        "        super(exVDPMLP, self).__init__()\n",
        "        self.linear_1 = LinearFirst(256)\n",
        "        self.myrelu_1 = myReLU()\n",
        "        self.linear_2 = LinearNotFirst(2)\n",
        "        self.mysoftma = mysoftmax()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        m, s = self.linear_1 (inputs)\n",
        "        m, s = self.myrelu_1 (m, s)\n",
        "        m, s = self.linear_2 (m, s)\n",
        "        outputs, Sigma = self.mysoftma(m, s)        \n",
        "        return outputs, Sigma\n",
        "\n",
        "def main_function( input_dim = 10, units = 256, output_size = 2 , batch_size = 200, epochs = 20, lr = 0.001, \n",
        "         Random_noise=True, gaussain_noise_std=1, Training = False):\n",
        "    \n",
        "\n",
        "    PATH = './saved_models/DP_MLP_epoch_{}/'.format(epochs)\n",
        "    X = np.load('./x_train.npy')\n",
        "    y = np.load('./y_train.npy')\n",
        "    x_train = X[0:120000,:]\n",
        "    y_train = y[0:120000]\n",
        "    x_test = X[120000:133600,:]\n",
        "    y_test = y[120000:133600]   \n",
        "\n",
        "    one_hot_y_train = tf.one_hot(y_train.astype(np.float32), depth=2)\n",
        "    one_hot_y_test = tf.one_hot(y_test.astype(np.float32), depth=2) \n",
        "\n",
        "    tr_dataset = tf.data.Dataset.from_tensor_slices((x_train, one_hot_y_train)).batch(batch_size)\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((x_test, one_hot_y_test)).batch(batch_size)\n",
        "\n",
        "        \n",
        "    # Cutom Trianing Loop with Graph\n",
        "    mlp_model = exVDPMLP(name='mlp')    \n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    \n",
        "    @tf.function  # Make it fast.\n",
        "    def train_on_batch(x, y):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits, sigma = mlp_model(x)      \n",
        "            loss_final = nll_gaussian(y, logits,  tf.clip_by_value(t=sigma, clip_value_min=tf.constant(-1e+10),\n",
        "                                       clip_value_max=tf.constant(1e+10)), output_size , batch_size)\n",
        "            loss_layers = sum(mlp_model.losses)\n",
        "\n",
        "            loss = loss_final + 0.001*loss_layers\n",
        "            gradients = tape.gradient(loss, mlp_model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(gradients, mlp_model.trainable_weights))        \n",
        "        return loss, logits\n",
        "    if Training:\n",
        "        train_acc = np.zeros(epochs) \n",
        "        valid_acc = np.zeros(epochs)\n",
        "        train_err = np.zeros(epochs)\n",
        "        valid_error = np.zeros(epochs)\n",
        "        start = timeit.default_timer()\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "          print('Epoch: ', epoch+1, '/' , epochs)\n",
        "    \n",
        "          acc1 = 0 \n",
        "          acc_valid1 = 0 \n",
        "          err1 = 0\n",
        "          err_valid1 = 0\n",
        "          tr_no_steps = 0          \n",
        "          #Training\n",
        "          for step, (x, y) in enumerate(tr_dataset):\n",
        "              update_progress(step / int(x_train.shape[0] / (batch_size)) )  \n",
        "              loss, logits = train_on_batch(x, y)\n",
        "              err1+= loss\n",
        "    \n",
        "              corr = tf.equal(tf.math.argmax(logits, axis=1),tf.math.argmax(y,axis=1))\n",
        "              accuracy = tf.reduce_mean(tf.cast(corr,tf.float32))\n",
        "              acc1+=accuracy                 \n",
        "              tr_no_steps+=1\n",
        "          train_acc[epoch] = acc1/tr_no_steps\n",
        "          train_err[epoch] = err1/tr_no_steps\n",
        "          \n",
        "          print('Training Acc  ', train_acc[epoch])\n",
        "          print('Training error  ', train_err[epoch])          \n",
        "          \n",
        "        stop = timeit.default_timer()\n",
        "        print('Total Training Time: ', stop - start)\n",
        "        print('Training Acc  ', np.mean(train_acc))          \n",
        "        print('Training error  ', np.mean(train_err))       \n",
        "    \n",
        "        \n",
        "        mlp_model.save_weights(PATH + 'DP_MLP_model')\n",
        "        \n",
        "        if (epochs > 1):\n",
        "            fig = plt.figure(figsize=(15,7))\n",
        "            plt.plot(train_acc, 'b', label='Training acc')            \n",
        "            plt.ylim(0, 1.1)\n",
        "            plt.title(\"Density Propagation MLP on MNIST Data\")\n",
        "            plt.xlabel(\"Epochs\")\n",
        "            plt.ylabel(\"Accuracy\")\n",
        "            plt.legend(loc='lower right')\n",
        "            plt.savefig(PATH + 'DP_MLP_on_MNIST_Data_acc.png')\n",
        "            plt.close(fig)\n",
        "    \n",
        "    \n",
        "            fig = plt.figure(figsize=(15,7))\n",
        "            plt.plot(train_err, 'b', label='Training error')            \n",
        "            plt.title(\"Density Propagation MLP on MNIST Data\")\n",
        "            plt.xlabel(\"Epochs\")\n",
        "            plt.ylabel(\"Error\")\n",
        "            plt.legend(loc='lower right')\n",
        "            plt.savefig(PATH + 'DP_MLP_on_MNIST_Data_error.png')\n",
        "            plt.close(fig)\n",
        "        f = open(PATH + 'training_validation_acc_error.pkl', 'wb')         \n",
        "        pickle.dump([train_acc, train_err], f)                                                   \n",
        "        f.close()         \n",
        "             \n",
        "             \n",
        "        textfile = open(PATH + 'Related_hyperparameters.txt','w')    \n",
        "        textfile.write(' Input Dimension : ' +str(input_dim))\n",
        "        textfile.write('\\n No Hidden Nodes : ' +str(units))\n",
        "        textfile.write('\\n Output Size : ' +str(output_size))\n",
        "        textfile.write('\\n No of epochs : ' +str(epochs))\n",
        "        textfile.write('\\n Learning rate : ' +str(lr))            \n",
        "        textfile.write(\"\\n---------------------------------\")\n",
        "        if Training: \n",
        "            textfile.write('\\n Total run time in sec : ' +str(stop - start))\n",
        "            if(epochs == 1):\n",
        "                textfile.write(\"\\n Averaged Training  Accuracy : \"+ str( train_acc))                  \n",
        "                textfile.write(\"\\n Averaged Training  error : \"+ str( train_err))               \n",
        "            else:\n",
        "                textfile.write(\"\\n Averaged Training  Accuracy : \"+ str(np.mean(train_acc)))               \n",
        "                textfile.write(\"\\n Averaged Training  error : \"+ str(np.mean(train_err)))               \n",
        "        textfile.write(\"\\n---------------------------------\")                \n",
        "        textfile.write(\"\\n---------------------------------\")    \n",
        "        textfile.close()\n",
        "        \n",
        "    else:\n",
        "        test_path = 'test_results_random_noise_{}/'.format(gaussain_noise_std)\n",
        "        mlp_model.load_weights(PATH + 'DP_MLP_model')\n",
        "        test_no_steps = 0\n",
        "        err_test = 0\n",
        "        acc_test = 0\n",
        "        true_x = np.zeros([int(x_test.shape[0] / (batch_size)), batch_size,  input_dim])\n",
        "        true_y = np.zeros([int(x_test.shape[0] / (batch_size)), batch_size, output_size])\n",
        "        logits_ = np.zeros([int(x_test.shape[0] / (batch_size)), batch_size, output_size])\n",
        "        sigma_ = np.zeros([int(x_test.shape[0] / (batch_size)), batch_size, output_size, output_size])\n",
        "        for step, (x, y) in enumerate(val_dataset):\n",
        "          update_progress(step / int(x_test.shape[0] / (batch_size)) ) \n",
        "          true_x[test_no_steps, :, :] = x\n",
        "          true_y[test_no_steps, :, :] = y\n",
        "          if Random_noise:\n",
        "              noise = tf.random.normal(shape = [batch_size, input_dim], mean = 0.0, stddev = gaussain_noise_std, dtype = x.dtype ) \n",
        "              x = x +  noise \n",
        "\n",
        "          loss_layers = sum(mlp_model.losses)       \n",
        "          logits, sigma = mlp_model(x)  \n",
        "          logits_[test_no_steps,:,:] =logits\n",
        "          sigma_[test_no_steps, :, :, :]= sigma\n",
        "          tloss = nll_gaussian(y, logits,  tf.clip_by_value(t=sigma, clip_value_min=tf.constant(-1e+10), clip_value_max=tf.constant(1e+10)), output_size , batch_size)+ 0.001*loss_layers\n",
        "          err_test+= tloss\n",
        "          \n",
        "          corr = tf.equal(tf.math.argmax(logits, axis=1),tf.math.argmax(y,axis=1))\n",
        "          accuracy = tf.reduce_mean(tf.cast(corr,tf.float32))\n",
        "          acc_test+=accuracy\n",
        "\n",
        "          if step % 500 == 0:\n",
        "              print(\"Step:\", step, \"Loss:\", float(tloss))\n",
        "              print(\"Total running accuracy so far: %.3f\" % accuracy)              \n",
        "           \n",
        "          test_no_steps+=1\n",
        "       \n",
        "        test_acc = acc_test/test_no_steps      \n",
        "        test_error = err_test/test_no_steps\n",
        "        print('Test accuracy : ', test_acc.numpy())\n",
        "        print('Test error : ', test_error.numpy())\n",
        "        \n",
        "        pf = open(PATH + test_path + 'uncertainty_info.pkl', 'wb')         \n",
        "        pickle.dump([logits_, sigma_, true_x, true_y, test_acc.numpy(), test_error.numpy()  ], pf)                                                   \n",
        "        pf.close()\n",
        "        \n",
        "        textfile = open(PATH + test_path + 'Related_hyperparameters.txt','w')    \n",
        "        textfile.write(' Input Dimension : ' +str(input_dim))\n",
        "        textfile.write('\\n No Hidden Nodes : ' +str(units))\n",
        "        textfile.write('\\n Output Size : ' +str(output_size))\n",
        "        textfile.write('\\n No of epochs : ' +str(epochs))\n",
        "        textfile.write('\\n Learning rate : ' +str(lr))          \n",
        "        textfile.write(\"\\n---------------------------------\")\n",
        "        textfile.write(\"\\n Averaged Test Accuracy : \"+ str( test_acc.numpy()))\n",
        "        textfile.write(\"\\n Averaged Test error : \"+ str(test_error.numpy() ))            \n",
        "        textfile.write(\"\\n---------------------------------\")\n",
        "        if Random_noise:\n",
        "            textfile.write('\\n Random Noise std: '+ str(gaussain_noise_std ))              \n",
        "        textfile.write(\"\\n---------------------------------\")    \n",
        "        textfile.close()\n",
        "            \n",
        "if __name__ == '__main__':\n",
        "    main_function()    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rPercent: [----------] 0.0% WARNING:tensorflow:Layer ex_vdpmlp_15 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Step: 0 Loss: -3.540673017501831\n",
            "Total running accuracy so far: 0.930\n",
            "Percent: [##########] 98.52941176470588% Test accuracy :  0.92830884\n",
            "Test error :  -4.916086\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}